--- START FILE: ./readme ---
Test
--- END FILE: ./readme ---
--- START FILE: ./.gitignore ---
data/*
data_R/*

*.pyc
*/__pycache__/*
*.h5
*.xdmf
*.txt
--- END FILE: ./.gitignore ---
--- START FILE: ./util/save.py ---
from typing import Any, cast
import json
import os
import numpy as np

class NumpyEncoder(json.JSONEncoder):
    def default(self, o: Any) -> Any:
        if isinstance(o, np.integer):
            return int(o)
        elif isinstance(o, np.floating):
            return float(o)
        elif isinstance(o, np.ndarray):
            return o.tolist()
        return super().default(o)

# save to format js line
def save_result(path: str, results: dict[str, Any]):
    results_str = json.dumps(results, cls=NumpyEncoder)
    
    # create directory if it does not exist
    os.makedirs(os.path.dirname(path), exist_ok=True)

    with open(path, 'a') as f:
        f.write(results_str + '\n')
    


def load_results(path: str) -> list[dict[str, Any]]:
    list_results: list[dict[str, Any]] = []
    with open(path, 'r') as f:
        for line in f:
            ljs = json.loads(line)

            if not isinstance(ljs, dict):
                raise ValueError(f"Invalid line in file: {line}")
            
            typed_ljs = cast(dict[str, Any], ljs)
            list_results.append(typed_ljs)
    return list_results


def save_results(path: str, results: list[dict[str, Any]]):
    os.makedirs(os.path.dirname(path), exist_ok=True)

    with open(path, 'a') as f:
        for result in results:
            result_str = json.dumps(result)
            f.write(result_str + '\n')


def conc_files(input_file: str, output_file: str):
    with open(input_file, 'r') as f_in, open(output_file, 'a') as f_out:
        for line in f_in:
            f_out.write(line)
--- END FILE: ./util/save.py ---
--- START FILE: ./util/database.py ---
import os
from util.data_generator import DataGenerator
from util.convert import C_converter, R_converter, D_converter

from typing import Any
import random as rd
import numpy as np
import math
import torch

def empty_database(folder: str):
    if os.path.exists(folder):
        for sub_folder in ["train", "test", "dev", "plot"]:
            sub_folder_path = os.path.join(folder, sub_folder)
            if os.path.exists(sub_folder_path):
                for file in os.listdir(sub_folder_path):
                    os.remove(os.path.join(sub_folder_path, file))
            else:
                os.makedirs(sub_folder_path)
    else:
        os.makedirs(folder)
        os.makedirs(os.path.join(folder, "train"))
        os.makedirs(os.path.join(folder, "test"))
        os.makedirs(os.path.join(folder, "dev"))
        os.makedirs(os.path.join(folder, "plot"))


def extract_params_from_brut(folder: str):
    list_params: list[dict[Any, Any]] = []
    for sub_folder in os.listdir(folder):

        if not os.path.isdir(os.path.join(folder, sub_folder)):
            continue

        try:
            C_out, C_in, T1_in, R_nm = map(float, sub_folder.split('_'))
        except ValueError:
            print(f"Skipping folder {sub_folder} due to ValueError")
            continue


        for file in os.listdir(os.path.join(folder, sub_folder)):

            P0, TB, Time, P = np.loadtxt(
                os.path.join(folder, sub_folder, file),
                comments='%',          # saute toutes les lignes qui commencent par %
                unpack=True            # renvoie 4 tableaux séparés
            )

            P0 = float(P0[0])
            TB = float(TB[0])


            # Convertir les valeurs
            C_out = C_converter(C_out)  # C_out from mol.L^-1 to Fmol.µm^3
            C_in = C_converter(C_in)  # C_in from mol.L^-1 to Fmol.µm^3

            R = R_converter(R_nm)  # R in µm

            D_ref = D_converter(500.)  # D_ref = 500 nm^2.s^-1
            C_ref = C_converter(60.)   # C_ref = 60 mol.L^-1      
            D_in = D_ref * (C_in / C_ref) ** (1/3)
            D_out = D_ref * (C_out/ C_ref) ** (1/3)


            T1_out = TB

            list_params.append({
                "C_out": C_out,
                "C_in": C_in,
                "D_out": D_out,
                "D_in": D_in,
                "T1_out": T1_out,
                "T1_in": T1_in,
                "P0_out": P0,
                "P0_in": 1., # car Boltzmann est 1
                "R": R, 
            })

    return list_params


def create_database(list_dict: list[dict[Any, Any]], folder: str, n=20, micro_ondes: bool= True, test: float = 0.1, dev: float = 0.1):

    R_max = max([params["R"] for params in list_dict]) # typ: ignore
    R_min = min([params["R"] for params in list_dict]) # typ: ignore

    C_out_max = max([params["C_out"] for params in list_dict]) # typ: ignore
    C_out_min = min([params["C_out"] for params in list_dict]) # typ: ignore
    C_in_max = max([params["C_in"] for params in list_dict]) # typ: ignore
    C_in_min = min([params["C_in"] for params in list_dict]) # typ: ignore

    D_out_max = max([params["D_out"] for params in list_dict]) # typ: ignore
    D_out_min = min([params["D_out"] for params in list_dict]) # typ: ignore
    D_in_max = max([params["D_in"] for params in list_dict]) # typ: ignore
    D_in_min = min([params["D_in"] for params in list_dict]) # typ: ignore

    T1_in_max = max([params["T1_in"] for params in list_dict]) # typ: ignore
    T1_in_min = min([params["T1_in"] for params in list_dict]) # typ: ignore

    #P0_out_max = max([params["P0_out"] for params in list_dict])
    #P0_out_min = min([params["P0_out"] for params in list_dict])
    #T1_out_max = max([params["T1_out"] for params in list_dict])
    #T1_out_min = min([params["T1_out"] for params in list_dict])
    #P0_in_max = max([params["P0_in"] for params in list_dict])
    #P0_in_min = min([params["P0_in"] for params in list_dict])


    if micro_ondes:
        P0_out = 100
        T1_out = 3.
    
    else:
        P0_out = 0.5
        T1_out = 3.5


    print(f"""{R_min} < R < {R_max} 
            {C_in_min} < C_in < {C_in_max:2f}, {C_out_min} < C_out < {C_out_max:2f} 
            {D_in_min:2f} < D_in < {D_in_max:2f}, {D_out_min:2f} < D_out < {D_out_max:2f} 
            {T1_in_min:2f} < T1_in < {T1_in_max:2f}, T1_out = {T1_out:2f} 
            P0_in = 1, P0_out = {P0_out:2f} 
    """)


    for _ in range(n):
        R =  rd.uniform(0.001, 0.5) 
        #rd.uniform(R_min, R_max)
        #math.exp(rd.uniform(math.log(R_min), math.log(R_max)))
        dg = DataGenerator(
            R=R,
            r_max=6*R,
            C_in=rd.uniform(C_in_min, C_in_max),
            C_out=rd.uniform(C_out_min, C_out_max),
            D_in=rd.uniform(D_in_min, D_in_max),
            D_out=rd.uniform(D_out_min, D_out_max),
            T1_in=rd.uniform(T1_in_min, T1_in_max),
            T1_out=T1_out,  #rd.uniform(T1_out_min, T1_out_max),
            P0_in=1., #rd.uniform(P0_in_min, P0_in_max),
            P0_out=P0_out, #rd.uniform(P0_out_min, P0_out_max),
            Tfinal=40.,
            Nr=200,
            Nt=100,
            tanh_slope=0
        )
        dg.solve()
        dg.plot(os.path.join(folder, "plot"))
        data = dg.get()

        if (a:=rd.random()) < test:
            sub_folder = "test"
        elif a < test + dev:
            sub_folder = "dev"
        else:
            sub_folder = "train"

        file_path = os.path.join(folder, sub_folder, f"data_{data['C_out']}_{data['C_in']}_{data['D_in']}_{data['D_out']}_{data['R']}.pt")
        torch.save(data, file_path)

        data["P"] = []
        print(f"{data=}")
        

        del dg
        del data

        
if __name__ == "__main__":
    #empty_database("data")
    list_dict = extract_params_from_brut("data/brut")
    create_database(list_dict, "data_R", test=0.1, dev=0.1, n=10_000, micro_ondes=True)
--- END FILE: ./util/database.py ---
--- START FILE: ./util/convert.py ---
def D_converter(D: float) -> float:
    """
    Convert D from nm^2.s^-1 to µm^2.s^-1
    """
    return D * 1e-6

def C_converter(C: float) -> float:
    """
    Convert C from mol.L^-1 to Fmol.µm^3
    """
    return C

def R_converter(R: float) -> float:
    """
    Convert R from nm to µm
    """
    return R * 1e-3
--- END FILE: ./util/convert.py ---
--- START FILE: ./util/data_generator.py ---
from mpi4py import MPI
import numpy as np
import ufl
from petsc4py.PETSc import ScalarType
from dolfinx import mesh, fem
from dolfinx.fem.petsc import LinearProblem


import torch
from torch import Tensor


from collections.abc import Callable
import os


class DataGenerator:
    def __init__(self,
                 R: float, r_max: float,
                 C_in: float, C_out: float,
                 D_in: float, D_out: float,
                 T1_in: float, T1_out: float,
                 P0_in: float, P0_out: float,
                 Tfinal: float = 10.,
                 Nr: int = 100,
                 Nt: int = 100,
                 tanh_slope: float = 0.,
                 ):

        self.R = R
        self.r_max = r_max
        self.Tfinal = Tfinal
        self.Nr = Nr
        self.Nt = Nt
        self.dt = self.Tfinal / self.Nt
        self.tanh_slope = tanh_slope

        self.C_in = C_in
        self.C_out = C_out
        self.D_in = D_in
        self.D_out = D_out
        self.T1_in = T1_in
        self.T1_out = T1_out
        self.P0_in = P0_in
        self.P0_out = P0_out

        if tanh_slope == 0.:
            self.C: Callable = lambda r: ufl.conditional(ufl.lt(r, self.R), C_in, C_out)
            self.D: Callable = lambda r: ufl.conditional(ufl.lt(r, self.R), D_in, D_out)
            self.T1: Callable = lambda r: ufl.conditional(ufl.lt(r, self.R), T1_in, T1_out)
            self.P0: Callable = lambda r: ufl.conditional(ufl.lt(r, self.R), P0_in, P0_out)

        else:
            mid   = 0.5*(C_out + C_in)
            amp   = 0.5*(C_out - C_in)
            self.C  = lambda r: mid + amp * ufl.tanh((r - self.R)/tanh_slope)

            mid   = 0.5*(D_out + D_in)
            amp   = 0.5*(D_out - D_in)
            self.D  = lambda r: mid + amp * ufl.tanh((r - self.R)/tanh_slope)

            mid   = 0.5*(T1_out + T1_in)
            amp   = 0.5*(T1_out - T1_in)
            self.T1 = lambda r: mid + amp * ufl.tanh((r - self.R)/tanh_slope)

            mid   = 0.5*(P0_out + P0_in)
            amp   = 0.5*(P0_out - P0_in)
            self.P0 = lambda r: mid + amp * ufl.tanh((r - self.R)/tanh_slope)
            #self.C: Callable = lambda r: ufl.tanh((r - self.R) / tanh_slope) * (C_out - C_in) + C_in
            #self.D: Callable = lambda r: ufl.tanh((r - self.R) / tanh_slope) * (D_out - D_in) + D_in
            #self.T1: Callable = lambda r: ufl.tanh((r - self.R) / tanh_slope) * (T1_out - T1_in) + T1_in
            #self.P0: Callable = lambda r: ufl.tanh((r - self.R) / tanh_slope) * (P0_out - P0_in) + P0_in


        self.msh = None
        self.V = None
        self.P_time = []
        self.r_sorted = None
        self.t_vec = None

    
    def solve(self):

        self.msh = mesh.create_interval(MPI.COMM_WORLD, self.Nr, [0.0, self.r_max])

        self.V = fem.functionspace(self.msh, ("Lagrange", 1))

        x = ufl.SpatialCoordinate(self.msh)[0]
        r = x
        w = r**2  # Jacobian for spherical coordinates

        C_expr = self.C(r)
        D_expr = self.D(r)
        T1_expr = self.T1(r)
        P0_expr = self.P0(r)

        # ----------------------------------------------------------
        # Variables for the problem
        # ----------------------------------------------------------

        P_n1 = ufl.TrialFunction(self.V)  # P^{n+1}
        v = ufl.TestFunction(self.V)  # Test function
        P_n = fem.Function(self.V)  # P^{n}

        P_n.x.array[:] = 0.0  # Initial condition: P(r, 0) = 0

        self.P_time.append(P_n)
        dx = ufl.dx

        # ----------------------------------------------------------
        # Weak form
        # ----------------------------------------------------------

        mass_lhs = (C_expr / ScalarType(self.dt)) * w * P_n1 * v * dx
        mass_rhs = (C_expr / ScalarType(self.dt)) * w * P_n * v * dx

        diff_lhs = D_expr * C_expr * w * ufl.dot(ufl.grad(P_n1), ufl.grad(v)) * dx

        react_lhs = (C_expr / T1_expr) * w * P_n1 * v * dx
        react_rhs = (C_expr / T1_expr) * w * P0_expr * v * dx

        a_form = mass_lhs + diff_lhs + react_lhs
        L_form = mass_rhs + react_rhs

        # ----------------------------------------------------------
        # Linear problem setup | Neumann comes from the weak form
        # ----------------------------------------------------------

        problem = LinearProblem(
            a_form, L_form, [], 
            petsc_options={"ksp_type": "preonly", "pc_type": "lu"}
        )


        n_loc = self.V.dofmap.index_map.size_local
        r_loc = self.V.tabulate_dof_coordinates()[:n_loc, 0]
        r_all = self.msh.comm.gather(r_loc, root=0)

        if self.msh.comm.rank == 0:
            r_glob = np.concatenate(r_all)
            order = np.argsort(r_glob)
            self.r_sorted = r_glob[order]
            P_hist = []

        t = 0.0
        for _ in range(self.Nt):
            t += self.dt

            P_new = problem.solve()

            P_loc = P_new.x.array[:n_loc]
            P_all = self.msh.comm.gather(P_loc, root=0)
            if self.msh.comm.rank == 0:
                P_glob = np.concatenate(P_all)
                P_hist.append(P_glob[order].copy())

            P_n.x.array[:] = P_new.x.array

        if self.msh.comm.rank == 0:
            self.P_time = np.array(P_hist)
            self.t_vec = np.linspace(self.dt, self.Tfinal, self.Nt)


    def plot(self, dir: str = "data/plot"):
        if self.msh.comm.rank == 0:
            if self.P_time is None or self.r_sorted is None or self.t_vec is None:
                print("No data to plot. Please run solve() first.")
                return

            import matplotlib.pyplot as plt
            os.makedirs(dir, exist_ok=True)

            plt.figure(figsize=(8, 5))
            plt.imshow(self.P_time,
                       extent=[0.0, self.r_max, 0.0, self.Tfinal],
                       origin="lower",
                       aspect="auto",
                       cmap="viridis")

            plt.colorbar(label=r"$P(r,t)$")
            plt.xlabel(r"$r$ (m)")
            plt.ylabel(r"$t$ (s)")
            plt.title("Évolution de la polarisation P(r,t)")
            plt.tight_layout()
            filename = f"{dir}/{self.R}_{self.C_in}_{self.C_out}_{self.D_in}_{self.D_out}_{self.P0_in}_{self.P0_out}_{self.T1_in}_{self.T1_out}.png"
            plt.savefig(filename, dpi=180)
            # plt.show()

    def get(self) -> dict[str, Tensor | float | int]:
        if self.msh.comm.rank == 0:
            P_tensor = torch.tensor(self.P_time, dtype=torch.float32)

            to_send: dict[str, Tensor | float | int] = {
                "P": P_tensor,
                "r_max": self.r_max,
                "Tfinal": self.Tfinal,
                "Nr": self.Nr,
                "Nt": self.Nt,
                "dt": self.dt,
                "C_in": self.C_in,
                "C_out": self.C_out,
                "D_in": self.D_in,
                "D_out": self.D_out,
                "T1_in": self.T1_in,
                "T1_out": self.T1_out,
                "P0_in": self.P0_in,
                "P0_out": self.P0_out,
                "R": self.R
            }
            return to_send
        return {}
        



--- END FILE: ./util/data_generator.py ---
--- START FILE: ./experiments/fno2d/train.py ---
import torch
import time

from neuralop import LpLoss

lp_loss = LpLoss(d=2, p=2, reduction="mean") 

def accuracy(model: torch.nn.Module,
            dataloader: torch.utils.data.DataLoader,
            device: torch.device) -> float:

    model.eval()

    total_error: float = 0.0

    with torch.no_grad():
        for a, u in dataloader:
            a, u = a.to(device), u.to(device)

            u_pred = model(a).squeeze(1)  # Assuming model outputs shape (batch_size, 1, Nt, Nr)
            #error: torch.Tensor = torch.nn.functional.mse_loss(u, u_pred)

            error = lp_loss(u_pred, u)
            total_error += error.item()

    return total_error / len(dataloader)



def train_one_epoch(model: torch.nn.Module, 
                    dataloader: torch.utils.data.DataLoader, 
                    optimizer: torch.optim.Optimizer, 
                    device: torch.device) -> float:
    
    model.train()

    total_loss: float = 0.0

    for a, u in dataloader:
        a, u = a.to(device), u.to(device)

        optimizer.zero_grad()

        u_pred = model(a).squeeze(1)  # Assuming model outputs shape (batch_size, 1, Nt, Nr)

        #loss: torch.Tensor = torch.nn.functional.mse_loss(u, u_pred)
        loss = lp_loss(u_pred, u)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    
    total_loss = total_loss / len(dataloader)

    return total_loss


def train(model: torch.nn.Module, 
          dataloader: torch.utils.data.DataLoader, 
          optimizer: torch.optim.Optimizer, 
          scheduler: torch.optim.lr_scheduler._LRScheduler,
          epochs: int, 
          device: torch.device,
          test_loader: torch.utils.data.DataLoader | None) -> tuple[list[float], list[float], list[float]]:

    model.to(device)

    
    

    train_losses: list[float] = []
    times: list[float] = []
    test_losses: list[float] = []

    for epoch in range(epochs):
        t0: float = time.time()
        loss: float = train_one_epoch(model, dataloader, optimizer, device)
        t1: float = time.time()
        train_losses.append(loss)
        times.append(t1 - t0)

        if test_loader is not None:
            test_loss: float = accuracy(model, test_loader, device)
            test_losses.append(test_loss)
            
            # Mettre à jour le scheduler avec la perte de test
            scheduler.step(loss)
            
            current_lr = optimizer.param_groups[0]['lr']
            print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}, Test Loss: {test_loss:.4f}, LR: {current_lr:.6f}, Time: {t1 - t0:.2f}s")
        
        else:
            # Sans test_loader, ReduceLROnPlateau ne peut pas fonctionner.
            # On peut appeler step() sur la perte d'entraînement, mais c'est moins courant.
            scheduler.step(loss)
            current_lr = optimizer.param_groups[0]['lr']
            print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}, LR: {current_lr:.6f}, Time: {t1 - t0:.2f}s")

    return train_losses, test_losses, times

--- END FILE: ./experiments/fno2d/train.py ---
--- START FILE: ./experiments/fno2d_R/main.py ---
from experiments.fno2d_R.dataset import Dataset
import torch
from torch import Tensor
import argparse
import os
import time
from neuralop.models import FNO
from experiments.fno2d.train import train, accuracy

from util.save import save_result

from sklearn.model_selection import train_test_split


# n_modes = 32
# hidden_channels = 64
# n_layers = 8


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
match os.cpu_count():
    case None:  torch.set_num_threads(1)
    case n:     torch.set_num_threads(n)

parser = argparse.ArgumentParser(description="PDE solving.")
parser.add_argument("-l", "--n_layers", type=int, default=2, help="number of layers")
parser.add_argument("-m", "--n_modes", type=int, default=16, help="number of modes")
parser.add_argument("-c", "--hidden_channels", type=int, default=16, help="number of hidden channels")
parser.add_argument("-e", "--epochs", type=int, default=100, help="number of training epochs")
parser.add_argument("-p", "--model_path", type=str, default="", help="path to model")
parser.add_argument("-d", "--dataset_path", type=str, default="data", help="path to dataset")
args = parser.parse_args()

n_modes = args.n_modes
hidden_channels = args.hidden_channels
n_layers = args.n_layers
epochs = args.epochs
dataset_path = args.dataset_path




if __name__ == "__main__":
    train_dataset = Dataset()
    train_dataset.load(
        os.path.join(dataset_path, "train")
        )

    train_dataset.rescale()
    train_dataset.nondimensionalize()
    train_dataset.compress()
    train_dataset.normalize()

    test_dataset = Dataset()
    #test_dataset.load("data/test")
    #test_dataset.normalize(dataset=train_dataset)

    train_dataset.elements, test_dataset.elements = train_test_split(
        train_dataset.elements,        # liste originale
        test_size=0.1, random_state=42 # ou stratifié si vous avez des labels
    
    )

    print("Vérif rapide (should ~0,~1)")
    print("train  P mean/std :", train_dataset[0][1].mean().item(),
                                train_dataset[0][1].std().item())
    print("test   P mean/std :",  test_dataset[0][1].mean().item(),
                                test_dataset[0][1].std().item())

    train_dataloader = train_dataset.get_dataloader(64, shuffle=True)
    test_dataloader = test_dataset.get_dataloader(64, shuffle=False)

    if not args.model_path:
        checkpoint = {
            "parameters": {
                "n_modes": (n_modes,n_modes),
                "hidden_channels": hidden_channels,
                "n_layers": n_layers,
                "lift_dropout": 0.1,
                "projection_dropout": 0.1,
                "in_channels": 3,
                "out_channels": 1,
            },

            "model_state_dict": None,
            "iterations": []
        }


        model = FNO(n_modes=(n_modes,n_modes),
                    hidden_channels=hidden_channels,
                    in_channels=3,
                    out_channels=1,
                    n_layers=n_layers,
                    lift_dropout=0.1, 
                    projection_dropout=0.1
        )

    else:
        checkpoint = torch.load(args.model_path)
        model = FNO(**checkpoint["parameters"])
        model.load_state_dict(checkpoint["model_state_dict"])

    model.to(device)

    print(f"Normalisation parameters: {train_dataset.C_normalizer=}, {train_dataset.D_normalizer=}, {train_dataset.T1_normalizer=}")

    print("Accuracy without training : ", accuracy(model, test_dataloader, device))

    

    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=1e-2,
        #betas=(0.9, 0.999),
        #eps=1e-8,
        weight_decay=1e-4
    )

    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5)



    train_losses, test_losses, times = train(model=model,
        dataloader=train_dataloader,
        optimizer=optimizer,
        scheduler=scheduler,
        epochs=epochs,
        device=device,
        test_loader=test_dataloader
    )

    iteration = {
        "train_losses": train_losses,
        "test_losses": test_losses,
        "times": times,
        "dataset_path": dataset_path
    }

    checkpoint["iterations"].append(iteration)
    checkpoint["model_state_dict"] = model.state_dict()

    torch.save(checkpoint, f"out/fno2d/results_{n_modes}_{hidden_channels}_{n_layers}_{epochs}_{len(iteration)}.pt")

    #save_result(f"out/fno2d/results_{n_modes}_{hidden_channels}_{n_layers}_{epochs}.json", iteration)





    

--- END FILE: ./experiments/fno2d_R/main.py ---
--- START FILE: ./experiments/fno2d_R/edp_parameters.py ---
from collections.abc import Callable
import math

from torch import Tensor

class Normalizer:
    def __init__(self, mean: float, std: float):
        self.mean = mean
        self.std = std

    def normalize(self, value: float | Tensor) -> float | Tensor:
        return (value - self.mean) / self.std
    
    def unnormalize(self, value: float | Tensor) -> float | Tensor:
        return value * self.std + self.mean

    def __str__(self):
        return f"(mean={self.mean}, std={self.std})"

class EDPParameters:
    def __init__(self, 
                 Nr: int, Nt: int, 
                 R: float, r_max: float, 
                 t_max: float,
                 C_in: float, C_out: float,
                 D_in: float, D_out: float,
                 T1_in: float, T1_out: float,
                 P0_in: float, P0_out: float,
                 parent: "EDPParameters" | None = None,
                 parenthood_label: str | None = None,
    ):
        self.Nr = Nr
        self.Nt = Nt
        self.R = R
        self.r_max = r_max
        self.t_max = t_max
        self.C_in = C_in
        self.C_out = C_out
        self.D_in = D_in
        self.D_out = D_out
        self.T1_in = T1_in
        self.T1_out = T1_out
        self.P0_in = P0_in
        self.P0_out = P0_out

        self.parent = parent
        self.parenthood_label = parenthood_label
    
    
    def rescaling(self) -> "EDPParameters":
        t_max = 1.0  

        r_max = 1.0
        R = self.R * (1/self.r_max)

        C_in = self.C_in * (self.r_max**3)
        C_out = self.C_out * (self.r_max**3)
        
        D_in = self.D_in * (self.t_max/self.r_max**2)
        D_out = self.D_out * (self.t_max/self.r_max**2)

        T1_in = self.T1_in * (1/self.t_max)
        T1_out = self.T1_out * (1/self.t_max)

        P0_in = self.P0_in
        P0_out = self.P0_out        

        return EDPParameters(
            Nr=self.Nr, Nt=self.Nt,
            R=R, r_max=r_max, t_max=t_max,
            C_in=C_in, C_out=C_out,
            D_in=D_in, D_out=D_out,
            T1_in=T1_in, T1_out=T1_out,
            P0_in=P0_in, P0_out=P0_out,
            parent=self,
            parenthood_label="rescaling"
        )
    

    def nondimensionalize(self) -> "EDPParameters":
        
        C_out = 1.0
        C_in = self.C_in * (1/self.C_out)

        return EDPParameters(
            Nr=self.Nr, Nt=self.Nt,
            R=self.R, r_max=self.r_max, t_max=self.t_max,
            C_in=C_in, C_out=C_out,
            D_in=self.D_in, D_out=self.D_out,
            T1_in=self.T1_in, T1_out=self.T1_out,
            P0_in=self.P0_in, P0_out=self.P0_out,
            parent=self,
            parenthood_label="nondimensionalize"
        )


    def compression(self) -> "EDPParameters":
        def C_compress(C: float) -> float:
            return math.log(C)
        
        def D_compress(D: float) -> float:
            return math.log(D)
        
        def R_compress(R: float) -> float:
            return math.log(R)
        

        return EDPParameters(
            Nr=self.Nr, Nt=self.Nt,
            R=R_compress(self.R), r_max=self.r_max, 
            t_max=self.t_max,
            C_in=C_compress(self.C_in), C_out=C_compress(self.C_out),
            D_in=D_compress(self.D_in), D_out=D_compress(self.D_out),
            T1_in=self.T1_in, T1_out=self.T1_out,
            P0_in=self.P0_in, P0_out=self.P0_out,
            parent=self,
            parenthood_label="compression"
        )
    

    def normalize(self, C_normalizer: Callable[[float], float] | None = None, D_normalizer: Callable[[float], float] | None = None, R_normalizer: Callable[[float], float] | None = None, T1_normalizer: Callable[[float], float] | None = None) -> "EDPParameters":
        
        if C_normalizer is None:
            C_normalizer = lambda x: x
        if D_normalizer is None:
            D_normalizer = lambda x: x
        if R_normalizer is None:
            R_normalizer = lambda x: x
        if T1_normalizer is None:
            T1_normalizer = lambda x: x
        
        return EDPParameters(
            Nr=self.Nr, Nt=self.Nt,
            R=R_normalizer(self.R), r_max=self.r_max, 
            t_max=self.t_max,
            C_in=C_normalizer(self.C_in), C_out=C_normalizer(self.C_out),
            D_in=D_normalizer(self.D_in), D_out=D_normalizer(self.D_out),
            T1_in=self.T1_in, T1_out=self.T1_out,
            P0_in=self.P0_in, P0_out=self.P0_out,
            parent=self,
            parenthood_label="normalize"
        )

    

if __name__ == "__main__":
    params = EDPParameters(
        Nr=100, Nt=100,
        R=1.0, r_max=1.0, t_max=1.0,
        C_in=1.0, C_out=1.0,
        D_in=1.0, D_out=1.0,
        T1_in=1.0, T1_out=1.0,
        P0_in=1.0, P0_out=1.0
    )

    # basic usage
    params_to_learn = params.rescaling().nondimensionalize().compression().normalize()

    print(params_to_learn)
--- END FILE: ./experiments/fno2d_R/edp_parameters.py ---
--- START FILE: ./experiments/fno2d_R/loss.py ---
import torch
from torch import Tensor
import torch.nn.functional as F

from torch.autograd import grad

from neuralop.losses.data_losses import central_diff_2d


class FickEqnLoss(object):
    

    def __init__(self, method: str="fdm", loss=F.mse_loss):
        super().__init__()
        self.method = method
        self.loss = loss

    def fdm(self, u: Tensor):
        # remove extra channel dimensions
        u = u.squeeze(1)

        # shapes
        _, nt, nx = u.shape


        R =     


    def __call__(self, y_pred, **kwargs):
        if self.method == "fdm":
            return self.fdm(u=y_pred)
        raise NotImplementedError()


--- END FILE: ./experiments/fno2d_R/loss.py ---
--- START FILE: ./experiments/fno2d_R/run.sh ---
#!/bin/bash
#SBATCH --array=1-270%3
#SBATCH --job-name=fno2d
#SBATCH --nodes=1                # nombre de noeuds
#SBATCH --ntasks=1               # nombre total de tâches sur tous les nœuds
#SBATCH --cpus-per-task=1
#SBATCH --time=48:00:00
#SBATCH --mem=4G
#SBATCH --output=hs_slurm/dcv_hist/out/slurm-%A_%a.txt
#SBATCH --error=hs_slurm/dcv_hist/err/slurm-%A_%a.txt
#SBATCH --mail-type=ALL
#SBATCH --requeue
#SBATCH --gres=gpu:1
#SBATCH --partition=gpu

# export TMPDIR=/scratch/<project>/tmp

DIR=$1
PARAMS_OFFSET=$2

PARAMS_FILE="${DIR}/params"
RUNLOG_FILE="${DIR}/runlog"
BATCH_HIST="batch.txt"

if [ -z "$PARAMS_OFFSET" ]
then
    PARAMS_OFFSET=0
fi

if [ ! -d "$DIR" -o ! -f "$PARAMS_FILE" ]
then
    echo "Usage: $0 DIR [PARAMS_OFFSET]"
    echo "where DIR is a directory containing a file 'params' with the parameters."
    exit 1
fi

PARAMS_ID=$(( $SLURM_ARRAY_TASK_ID + $PARAMS_OFFSET ))
JOB_NAME="${SLURM_ARRAY_JOB_ID}_${SLURM_ARRAY_TASK_ID}"

echo "$PARAMS_ID|$JOB_NAME|$SLURM_SUBMIT_DIR" >> $RUNLOG_FILE

PARAMS=$(tail -n +${PARAMS_ID} ${PARAMS_FILE} | head -n 1)

CMD=$"srun python -m experiments.fno2d_R.main ${PARAMS}"

echo "start"
ml python/3.12
ml cuda/12.4
echo "$PARAMS"
source ../env/bin/activate 
export PYTHONPATH=$PWD 
echo "$PARAMS_ID|$JOB_NAME|$SLURM_SUBMIT_DIR|$CMD" >> $BATCH_HIST
$CMD
deactivate
echo "end"
--- END FILE: ./experiments/fno2d_R/run.sh ---
--- START FILE: ./experiments/fno2d_R/dataset.py ---
from mpi4py import MPI
import numpy as np
import ufl
from petsc4py.PETSc import ScalarType
from dolfinx import mesh, fem
from dolfinx.fem.petsc import LinearProblem
import torch
from torch import Tensor
from collections.abc import Callable
import os


class DataGenerator:
    def __init__(self,
                 R: float, r_max: float,
                 C_in: float, C_out: float,
                 D_in: float, D_out: float,
                 T1_in: float, T1_out: float,
                 P0_in: float, P0_out: float,
                 Tfinal: float = 10.,
                 Nr: int = 100,
                 Nt: int = 100,
                 tanh_slope: float = 0.,
                 ):

        self.R = R
        self.r_max = r_max
        self.Tfinal = Tfinal
        self.Nr = Nr
        self.Nt = Nt
        self.dt = self.Tfinal / self.Nt
        self.tanh_slope = tanh_slope

        self.C_in = C_in
        self.C_out = C_out
        self.D_in = D_in
        self.D_out = D_out
        self.T1_in = T1_in
        self.T1_out = T1_out
        self.P0_in = P0_in
        self.P0_out = P0_out

        if tanh_slope == 0.:
            self.C: Callable = lambda r: ufl.conditional(ufl.lt(r, self.R), C_in, C_out)
            self.D: Callable = lambda r: ufl.conditional(ufl.lt(r, self.R), D_in, D_out)
            self.T1: Callable = lambda r: ufl.conditional(ufl.lt(r, self.R), T1_in, T1_out)
            self.P0: Callable = lambda r: ufl.conditional(ufl.lt(r, self.R), P0_in, P0_out)
        else:
            mid = 0.5 * (C_out + C_in)
            amp = 0.5 * (C_out - C_in)
            self.C = lambda r: mid + amp * ufl.tanh((r - self.R) / tanh_slope)
            mid = 0.5 * (D_out + D_in)
            amp = 0.5 * (D_out - D_in)
            self.D = lambda r: mid + amp * ufl.tanh((r - self.R) / tanh_slope)
            mid = 0.5 * (T1_out + T1_in)
            amp = 0.5 * (T1_out - T1_in)
            self.T1 = lambda r: mid + amp * ufl.tanh((r - self.R) / tanh_slope)
            mid = 0.5 * (P0_out + P0_in)
            amp = 0.5 * (P0_out - P0_in)
            self.P0 = lambda r: mid + amp * ufl.tanh((r - self.R) / tanh_slope)

        self.msh = None
        self.V = None
        self.P_time = []
        self.r_sorted = None
        self.t_vec = None

    def solve(self):
        self.msh = mesh.create_interval(MPI.COMM_WORLD, self.Nr, [0.0, self.r_max])
        self.V = fem.functionspace(self.msh, ("Lagrange", 1))
        x = ufl.SpatialCoordinate(self.msh)[0]
        r = x
        w = r**2
        C_expr = self.C(r)
        D_expr = self.D(r)
        T1_expr = self.T1(r)
        P0_expr = self.P0(r)
        P_n1 = ufl.TrialFunction(self.V)
        v = ufl.TestFunction(self.V)
        P_n = fem.Function(self.V)
        P_n.x.array[:] = 0.0
        self.P_time.append(P_n)
        dx = ufl.dx
        mass_lhs = (C_expr / ScalarType(self.dt)) * w * P_n1 * v * dx
        mass_rhs = (C_expr / ScalarType(self.dt)) * w * P_n * v * dx
        diff_lhs = D_expr * C_expr * w * ufl.dot(ufl.grad(P_n1), ufl.grad(v)) * dx
        react_lhs = (C_expr / T1_expr) * w * P_n1 * v * dx
        react_rhs = (C_expr / T1_expr) * w * P0_expr * v * dx
        a_form = mass_lhs + diff_lhs + react_lhs
        L_form = mass_rhs + react_rhs
        problem = LinearProblem(
            a_form, L_form, [],
            petsc_options={"ksp_type": "preonly", "pc_type": "lu"}
        )
        n_loc = self.V.dofmap.index_map.size_local
        r_loc = self.V.tabulate_dof_coordinates()[:n_loc, 0]
        r_all = self.msh.comm.gather(r_loc, root=0)
        if self.msh.comm.rank == 0:
            r_glob = np.concatenate(r_all)
            order = np.argsort(r_glob)
            self.r_sorted = r_glob[order]
            P_hist = []
        t = 0.0
        for _ in range(self.Nt):
            t += self.dt
            P_new = problem.solve()
            P_loc = P_new.x.array[:n_loc]
            P_all = self.msh.comm.gather(P_loc, root=0)
            if self.msh.comm.rank == 0:
                P_glob = np.concatenate(P_all)
                P_hist.append(P_glob[order].copy())
            P_n.x.array[:] = P_new.x.array
        if self.msh.comm.rank == 0:
            self.P_time = np.array(P_hist)
            self.t_vec = np.linspace(self.dt, self.Tfinal, self.Nt)

    def plot(self, dir: str = "data/plot"):
        if self.msh.comm.rank == 0:
            if self.P_time is None or self.r_sorted is None or self.t_vec is None:
                return
            import matplotlib.pyplot as plt
            os.makedirs(dir, exist_ok=True)
            plt.figure(figsize=(8, 5))
            plt.imshow(self.P_time,
                       extent=[0.0, self.r_max, 0.0, self.Tfinal],
                       origin="lower",
                       aspect="auto",
                       cmap="viridis")
            plt.colorbar(label=r"$P(r,t)$")
            plt.xlabel(r"$r$ (m)")
            plt.ylabel(r"$t$ (s)")
            plt.title("Évolution de la polarisation P(r,t)")
            plt.tight_layout()
            filename = f"{dir}/{self.R}_{self.C_in}_{self.C_out}_{self.D_in}_{self.D_out}_{self.P0_in}_{self.P0_out}_{self.T1_in}_{self.T1_out}.png"
            plt.savefig(filename, dpi=180)

    def get(self) -> dict[str, Tensor | float | int]:
        if self.msh.comm.rank == 0:
            P_tensor = torch.tensor(self.P_time, dtype=torch.float32)
            r = self.r_sorted
            idx_cut = np.searchsorted(r, self.R, side='right')
            G_vals = []
            for line in self.P_time:
                r_sub = r[:idx_cut]
                P_sub = line[:idx_cut]
                if r_sub[-1] < self.R:
                    if idx_cut < len(r):
                        r1, r2 = r[idx_cut - 1], r[idx_cut]
                        P1, P2 = line[idx_cut - 1], line[idx_cut]
                        P_R = P1 + (P2 - P1) * (self.R - r1) / (r2 - r1)
                    else:
                        P_R = P_sub[-1]
                    r_sub = np.append(r_sub, self.R)
                    P_sub = np.append(P_sub, P_R)
                G = 3.0 / (self.R ** 3) * np.trapz(P_sub * (r_sub ** 2), r_sub)
                G_vals.append(G)
            G_tensor = torch.tensor(G_vals, dtype=torch.float32)
            to_send: dict[str, Tensor | float | int] = {
                "P": P_tensor,
                "G_R": G_tensor,
                "r_max": self.r_max,
                "Tfinal": self.Tfinal,
                "Nr": self.Nr,
                "Nt": self.Nt,
                "dt": self.dt,
                "C_in": self.C_in,
                "C_out": self.C_out,
                "D_in": self.D_in,
                "D_out": self.D_out,
                "T1_in": self.T1_in,
                "T1_out": self.T1_out,
                "P0_in": self.P0_in,
                "P0_out": self.P0_out,
                "R": self.R
            }
            return to_send
        return {}

--- END FILE: ./experiments/fno2d_R/dataset.py ---
