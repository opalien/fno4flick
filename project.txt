--- START FILE: ./readme ---
\documentclass[11pt,a4paper]{article}

% Essential packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{amsthm}

\newtheorem{definition}{Définition}
\newtheorem{remark}{Remarque}
\theoremstyle{plain} 
\newtheorem{theorem}{Théorème} 
\newtheorem{example}{Exemple}
\newtheorem{properties}{Propriétés}
\newtheorem{proposition}{Proposition}
\newtheorem{preuve}{Preuve}

% Page layout
\geometry{margin=1in}

% Title and author information
\title{remember}
\author{Your Name\\
Your Institution\\
\texttt{your.email@institution.edu}}
\date{\today}

\begin{document}

\maketitle



\section{Introduction}
\label{sec:introduction}



%$$ \frac{\partial P(r, t)}{\partial t} = D(r) \frac{\partial^2 P(r, t)}{\partial r^2} + \left(\frac{2}{r} D(r) + \frac{D(r)}{C(r)} \frac{\partial C(r)}{\partial r} + \frac{\partial D(r)}{\partial r}\right) \frac{\partial P(r, t)}{\partial r} - \frac{P(r, t) - P_o(r)}{T_1(r)}$$
%
%$$ \forall r \in [0, R_{out}], P(r, 0) = 0$$
%$$ \forall t \in [0, T], \frac{\partial P(0, t)}{\partial r} = 0$$
%$$ \forall t \in [0, T], \frac{\partial P(R_{out}, t)}{\partial r} = 0$$
%
%
%Ce qui se réécrit en coordonées sphériques (en enlevant les angles car considérés symétriques) :
%
%
%
%
%$$ C(r) \frac{\partial P(r, t)}{\partial t} = \nabla \cdot (D(r) C(r) \nabla P(r, t)) - C(r) \frac{P(r, t)-P_0(r)}{T_1(r)}$$
%
%Ce qui se réécrit en coordonées sphériques (en enlevant les angles car considérés symétriques) :
%
%
%
%Soient $C(r)$ variable fonction de $r$ la concentration en electrons.
%Soit $D(r)$ la diffusion en fonction de $r$.
%Soit $P_0(r)$ le temps de relaxation.
%
%
%\begin{align}
%    \frac{\partial P(r, t)}{\partial t} ={}& D(r) \frac{\partial^2 P(r, t)}{\partial r^2} \nonumber \\
%    & + \left(\frac{2D(r)}{r} + \frac{D(r)}{C(r)} \frac{\partial C(r)}{\partial r} + \frac{\partial D(r)}{\partial r}\right) \frac{\partial P(r, t)}{\partial r} \nonumber \\
%    & - \frac{P(r, t) - P_o(r)}{T_1(r)} \label{eq:pde} \\
%    \label{eq:initial_cond}
%    P(r, 0) ={}& 0, &&\forall r \in [0, R_{\text{out}}] \\
%    \label{eq:boundary_cond_1}
%    \frac{\partial P(0, t)}{\partial r} ={}& 0, &&\forall t \in [0, T] \\
%    \label{eq:boundary_cond_2}
%    \frac{\partial P(R_{\text{out}}, t)}{\partial r} ={}& 0, &&\forall t \in [0, T]
%\end{align}

%$$ C(x) \frac{\partial P(x, t)}{\partial t} = \nabla \cdot (D(x) C(x) \nabla P(x, t)) - C(x) \frac{P(x, t)-P_0(x)}{T_1(x)} $$


\section{modélisation}

\section{Formulation du problème}


Soient :
\begin{itemize}
    \item $\mathcal{H}$ un espace vectoriel de dimension $3$ muni de la norme $\|\cdot \|_2$.
    \item $\Omega = \left\{ x \in \mathcal{H} | \|x\|_2 \leq R_{out} \right\}$ une boule fermée
    \item $n(x) = \frac{x}{\|x\|_2}$ le vecteur normal unitaire pour $x \in \partial\Omega$.
    \item Les fonctions suivantes définies sur $\Omega$ :
    \begin{itemize}
        \item $C(x)$ la concentration en électrons. $(mol.m^{-3})$
        \item $D(x)$ le coefficient de diffusion. $(m^2.s^{-1})$
        \item $P_0(x)$ la polarisation d'équilibre. (sans dimension)
        \item $T_1(x)$ le temps de relaxation. $(s)$
    \end{itemize}
    
\end{itemize}


On cherche à résoudre l'équation différentielle partielle de Flick suivante :


\begin{align}
    &C(x) \frac{\partial P(x, t)}{\partial t} = 
    \nabla \cdot (D(x) C(x) \nabla P(x, t)) 
    - C(x) \frac{P(x, t)-P_0(x)}{T_1(x)} 
    && \forall (x, t) \in \Omega \times [0,+\infty[ \label{eq:pde} \\
    \label{eq:initial_cond}
    &P(x, 0) = 0&& \forall x \in \Omega \\
    %\label{eq:boundary_cond_2}
    %&\frac{\partial P(0, t)}{\partial x} =0 && \forall t \in [0,+\infty[ \\
    \label{eq:boundary_cond_1}
    &\nabla P(x, t) \cdot n(x) =0 &&  \forall (x, t) \in \partial\Omega \times [0, +\infty[
\end{align}


\begin{proposition}
    Nous considérons que la solution possède cette propriété:
    $$ \forall x_1, x_2 \in \Omega, \|x_1 \|_2 = \|x_2\|_2 \implies P(x_1, t) = P(x_2, t)$$
\end{proposition}

\begin{proof}
    
\end{proof}

Par soucis de simplification, nous réécrivons l'équation \eqref{eq:pde} en coordonnées sphériques, en considérant que les angles sont symétriques, ce qui nous permet de ne garder que la variable $r = \|x\|_2$ :

\begin{align}
    &C(r) \frac{\partial P(r, t)}{\partial t} = 
    \nabla \cdot \left(D(r) C(r) \frac{\partial}{\partial r} P(r, t)\right) 
    - C(r) \frac{P(r, t)-P_0(r)}{T_1(r)} 
    && \forall (r, t) \in [0, R_{\text{out}}] \times [0,+\infty[ \label{eq:pde_spherique} \\
    %\label{eq:initial_cond_spherique}
    %&P(r, 0) = 0&& \forall r \in [0, R_{\text{out}}] \\
    %\label{eq:boundary_cond_1_spherique}
    %&\frac{\partial }{\partial r}P(R_{out}, t) =0 &&  \forall  t \in [0, +\infty[\\
    %\label{eq:boundary_cond_2_spherique}
    %&\frac{\partial }{\partial r}P(0, t) =0 && \forall t \in [0,+\infty[ 
\end{align}

Nous introduisons ici une nouvelle condition de bord pour $r = 0$.

Nous simplifions l'équation \eqref{eq:pde_spherique} :

\begin{align}
    \nabla \cdot \left(D(r) C(r) \frac{\partial P(r, t)}{\partial r} \right) 
    &= \frac{1}{r^2} \frac{\partial}{\partial r} \left(r^2 D(r) C(r) \frac{\partial P(r, t)}{\partial r}\right) \\
    &= D(r) C(r) \frac{\partial^2 P(r, t)}{\partial r^2} + \left(\frac{2}{r} D(r) C(r) + D(r)\frac{\partial C(r)}{\partial r} + C(r)\frac{\partial D(r)}{\partial r}\right) \frac{\partial P(r, t)}{\partial r}
\end{align}

Ce qui donne finalement l'équation différentielle partielle  linéaire suivante :

\begin{align}
    \frac{\partial P(r, t)}{\partial t} =& D(r) \frac{\partial^2 P(r, t)}{\partial r^2} \nonumber \\
    &+ \left( \frac{2}{r}D(r) +  \frac{D(r)}{C(r)} \frac{\partial C(r)}{\partial r} + \frac{\partial D(r)}{\partial r} \right) \frac{\partial P(r, t)}{\partial r} \nonumber \\
    &-\frac{P(r, t) - P_0(r)}{T_1(r)} \label{eq:pde_final}
\end{align}
avec les conditions :
\begin{align}
    &P(r, 0) = 0 && \forall r \in [0, R_{\text{out}}] \label{eq:initial_cond_final} \\
    &\frac{\partial P}{\partial r}(0, t) = 0 && \forall t \in [0, +\infty[ \label{eq:boundary_cond_0_final} \\
    &\frac{\partial P}{\partial r}(R_{out}, t) =0 && \forall t \in [0,+\infty[ 
\end{align}


\newpage

\section{résolution numérique}

\subsection{Formulation variationnelle}

Afin de résoudre le problème numériquement, nous mettons l'équation sous forme variationnelle :
en introduisant la fonction test $v(x)$, nous multiplions l'équation \eqref{eq:pde} par $v(x)$ et intégrons sur $\Omega$ :

$$ C(x) \frac{\partial P(x, t)}{\partial t} = 
    \nabla \cdot \left(D(x) C(x) \nabla P(x, t)\right) 
    - C(x) \frac{P(x, t)-P_0(x)}{T_1(x)} 
$$

$$ 
\Rightarrow \int_{\Omega} C(x) \frac{\partial P(x, t)}{\partial t} v(x) \, dx = \int_{\Omega} \nabla \cdot \left(D(x) C(x) \nabla P(x, t)\right) v(x) \, dx - \int_{\Omega} C(x) \frac{P(x, t)-P_0(x)}{T_1(x)} v(x) \, dx
$$

or par l'intégration par parties, nous avons sur le terme de diffusion :

$$
\int_{\Omega} \nabla \cdot \left(D(x) C(x) \nabla P(x, t)\right) v(x) \, dx
= \int_{\Omega} \nabla \cdot \left(D(x) C(x) \nabla P(x, t) v(x)\right) \, dx - \int_{\Omega} \left(D(x) C(x) \nabla P(x, t)\right) \cdot \nabla v(x) \, dx
$$

Par le théorème de Gauss-Ostrogradski, le premier terme devient :

$$
\int_{\Omega} \nabla \cdot \left(D(x) C(x) \nabla P(x, t) v(x)\right) \, dx 
= \int_{\partial\Omega} D(x) C(x) \underbrace{\nabla P(x, t) \cdot n(x)}_{= 0 \ \text{car condition au bord}} v(x) \, dS 
= 0
$$

Ainsi la forme faible est donc :
$$
\int_{\Omega} C(x) \frac{\partial P(x, t)}{\partial t} v(x) \, dx = - \int_{\Omega} \left(D(x) C(x) \nabla P(x, t)\right) \cdot \nabla v(x) \, dx - \int_{\Omega} C(x) \frac{P(x, t)-P_0(x)}{T_1(x)} v(x) \, dx
$$

\subsection{Discrétisation temporelle (Euler)}

Nous utilisons une approximation d'Eurler implicite pour la dérivée temporelle au temps discret $t_n$ avec $P^n \approx P(x, t_n)$ :

$$ \frac{\partial P(x, t_n)}{\partial t} = \frac{P^{n+1}(x) - P^n(x)}{\Delta t} $$

La formuation faible devient alors :

$$
\int_{\Omega} C(x) \frac{P^{n+1}(x) - P^n(x)}{\Delta t} v(x) \, dx 
+ \int_{\Omega} D(x) C(x) \nabla P^n(x) \cdot \nabla v(x) \, dx 
+ \int_{\Omega} \frac{C(x)}{T_1(x)} P^n v(x) \, dx
= \int_{\Omega} \frac{C(x)}{T_1(x)} P_0(x) v(x) \, dx
$$

\subsection{Formulation en coordonnées sphériques}

Pour chaque terme, l'on a :

\begin{align*}
    &\int_{\Omega} C(x) \frac{P^{n+1}(x) - P^n(x)}{\Delta t} v(x) \, dx 
    &&= 4 \pi \int_0^{R_{out}} C(r) \frac{P^{n+1}(r) - P^n(r)}{\Delta t} v(r) r^2 \, dr \\
    &\int_{\Omega} D(x) C(x) \nabla P^n(x) \cdot \nabla v(x) \, dx 
    &&= 4 \pi \int_0^{R_{out}} D(r) C(r) \frac{\partial P^n(r)}{\partial r} \frac{\partial v(r)}{\partial r} r^2 \, dr \\
    &\int_{\Omega} \frac{C(x)}{T_1(x)} P^n v(x) \, dx
    &&= 4 \pi \int_0^{R_{out}} \frac{C(r)}{T_1(r)} P^n(r) v(r) r^2 \, dr \\
    &\int_{\Omega} \frac{C(x)}{T_1(x)} P_0(x) v(x) \, dx 
    &&= 4 \pi \int_0^{R_{out}} \frac{C(r)}{T_1(r)} P_0(r) v(r) r^2 \, dr
\end{align*}

En remplaçant dans la formulation faible, nous obtenons :

\begin{align*}
    &4 \pi\int_0^{R_{out}} \frac{C(r)}{\Delta t} \left( P^{n+1}(r) - P^n(r) \right) v(r) r^2 \, dr 
    + 4 \pi\int_0^{R_{out}} D(r) C(r) \frac{\partial P^n(r)}{\partial r} \frac{\partial v(r)}{\partial r} r^2 \, dr 
    + 4 \pi\int_0^{R_{out}} \frac{C(r)}{T_1(r)} P^n(r) v(r) r^2 \, dr\\
    &= 4\pi \int_0^{R_{out}} \frac{C(r)}{T_1(r)} P_0(r) v(r) r^2 \, dr.
\end{align*}

Enfin, nous réalisons un changement de variable $\rho = \frac{r}{R_{out}}$ pour obtenir une meilleur stabilité numérique et nous supprimons le facteur $4\pi R_{out}^2$ 

\begin{align*}
    &\int_0^1 \frac{C(R_{out} \rho)}{\Delta t} \left( P^{n+1}(R_{out} \rho) - P^n(R_{out} \rho) \right) v(R_{out} \rho) \rho \, d\rho \\
    &+ \int_0^1 D(R_{out} \rho) C(R_{out} \rho) \frac{\partial P^n(R_{out} \rho)}{\partial r} \frac{\partial v(R_{out} \rho)}{\partial r} \rho \, d\rho \\
    &+ \int_0^1 \frac{C(R_{out} \rho)}{T_1(R_{out} \rho)} P^n(R_{out} \rho) v(R_{out} \rho) \rho \, d\rho \\
    &= \int_0^1 \frac{C(R_{out} \rho)}{T_1(R_{out} \rho)} P_0(R_{out} \rho) v(R_{out} \rho) \rho \, d\rho.
\end{align*}

\subsection{Dimensions}

Nous choisissons les micro mètres ($\mu m$), les femto moles ($Fmol$) et les secondes ($s$)


\section{Polarisation Moyenne}

Nous introduisons à présent la fonction représentant la polarisation moyenne :

$$
G(V, t) = \frac{1}{V} \int_{V} P(x, t) \, dx
$$

Nous pouvons réécrire cela en coordonnées sphériques :

$$
G(r, t) = \frac{1}{\frac{4}{3} \pi r^3} \int_0^r P(\mu, t) 4 \pi \mu^2 \, d\mu
= \frac{3}{r^3} \int_0^r P(\mu, t) \mu^2 \, d\mu
$$

\begin{proposition}
    Nous pouvons réécrire $P$ en fonction de $G$ en coordonnées sphériques :

    $$P(r, t) = \frac{r}{3}\frac{d}{dr} G(r, t) + G(r, t)$$
\end{proposition}

\begin{proof}
    Nous calculons :

    $$ \frac{d}{dr} G(r, t) = \frac{d}{dr} \left( \frac{3}{r^3} \int_0^r P(\mu, t) \mu^2 \, d\mu \right)
    = -\frac{9}{r^4} \int_0^r P(\mu, t) \mu^2 \, d\mu + \frac{3}{r^3} P(r, t) r^2
    = -\frac{3}{r} G(r, t) + \frac{3}{r} P(r, t)
    $$

    Ce qui donne :

    $$ P(r, t) = \frac{r}{3} \frac{d}{dr} G(r, t) + G(r, t) $$
\end{proof}


\subsection{forme faible pour la polarisation moyenne}

Réécrivons l'equation différentielle partielle \eqref{eq:pde_final} en fonction de $G$ :

\begin{align}
    C(r) \frac{\partial }{\partial t} \left( G(r, t) + \frac{r}{3} \frac{\partial}{\partial r} G(r, t) \right)
    =& \frac{1}{r^2} \frac{\partial}{\partial r} \left( r^2 D(r) C(r) \frac{4}{3} \frac{\partial}{\partial r} G(r, t) + \frac{r}{3}\frac{\partial^2}{\partial r^2} G(r, t) \right) \\
     &- \frac{C(r)}{T_1(r)} \left( G(r, t) + \frac{r}{3} \frac{\partial}{\partial r} G(r, t) - P_0(r) \right)
\end{align}

Nous multiplions par la fonction test $v(r)$ et intégrons sur $[0, R_{out}]$ :

\begin{align*}
    \int_{0}^{R_{out}} C \partial_t (G + \frac{r}{3} G_r) v r^2 \, dr =
    &\int_{0}^{R_{out}} \frac{\partial}{\partial r} \left( r^2 D C \frac{4}{3} G_r + \frac{r}{3} G_{rr} \right) v \, dr \\
    &- \int_{0}^{R_{out}} \frac{C}{T_1} \left( G + \frac{r}{3} G_r - P_0 \right) v r^2 \, dr
\end{align*}


Nous appliquons l'intégration par parties sur le terme de diffusion :

\begin{align*}
    \int_{0}^{R_{out}} \partial_r \left[r^2 D C(\tfrac{4}{3}G_r + \tfrac{r}{3}G_{rr})\right] v \, dr &= - \int_{0}^{R_{out}} r^2 D C(\tfrac{4}{3}G_r + \tfrac{r}{3}G_{rr}) v_r \, dr \quad (\text{bords} = 0 \text{ car } G_r = 0 \text{ sur } \partial\Omega) \\
    &= - \int_{0}^{R_{out}} r^2 D C(\tfrac{4}{3}G_r v_r + \tfrac{r}{3}G_{rr} v_r) \, dr.
\end{align*}

Nous supprimons la dérivée d'ordre $2$ en réintégrant par parties :

\begin{align*}
    \int_0^{R_{out}} r^3 D C G_{rr} v_r \, dr 
    &= - \int_{0}^{R_{out}} \partial_r \left( r^3 D C v_r \right) G_r \, dr
\end{align*}


Ce qui  donne finalement la forme faible pour la polarisation moyenne :


\section{Modèle FNO}

Nous donnons en entrée du FNO des cartes contenant les valeurs de $C(r, t)$, $D(r, t)$, $T_1(r, t)$ et $P_0(r, t)$.
Mais étant donnée que ces valeurs sont constantes en fonction du temps, et par soucis de clareté, nous n'écrirons que $C(r)$, $D(r)$, $T_1(r)$ et $P_0(r)$.

L'image de sortie est une carte contenant les valeurs de $P(r, t)$ sur l'intervalle $[0, 6R] \times [0, T_{max}]$.

\subsection{Echelle arbitraire}

Nous rendons le modèle invariable par $R$ et $T_{max}$ en réalisant un changement de variable pour chaque pair entrée sortie.
Cela permet de réduire l'espace de recherche tout en augmentant la généralisation à tout ordre de grandeur de $R$ et $T_{max}$.
Soit $L$ designant une mesure de longueur, $T$ une mesure de temps et $N$ la mesure de la quantité de matière.

(Considérons ici que Toutes le variables sont sans dimensions, et nous leur en attribuons une par l'opération de multiplication pour simplifier la notation.)


Nous posons $\hat{L}$, $\hat{T}$ et $\hat{N}$ les nouvelles unités telles que :

\begin{align*}
    & r.L = \hat{r}.\hat{L} \\
    & t.T = \hat{t}.\hat{T} \\
    & n.N = \hat{n}.\hat{N}
\end{align*}

or nous désirons que :

\begin{align*}
                & \hat{r}_{max} = 1 \\
    \Rightarrow & r_{max}. \frac{L}{\hat{L}} = 1 \\
    \Rightarrow & \frac{\hat{L}}{L} = r_{max} \\
\end{align*}

et de même pour le temps : 

\begin{align*}
                & \hat{t}_{max} = 1 \\
    \Rightarrow & t_{max}. \frac{T}{\hat{T}} = 1 \\
    \Rightarrow & \frac{\hat{T}}{T} = t_{max} \\
\end{align*}

Ce qui donne pour les autres variables :

\begin{align*}
    C(r) \frac{N}{L^3} = \hat{C}(\hat{r}) \frac{\hat{N}}{\hat{L}^3} 
    && \Rightarrow && 
    \hat{C}(\hat{r}) 
    = C(r) \frac{N}{L^3} \cdot \frac{\hat{L}^3}{\hat{N}}
    = C(r) \cdot r_{max}^3 \\
    D(r) \frac{L^2}{T} = \hat{D}(\hat{r}) \frac{\hat{L}^2}{\hat{T}}
    && \Rightarrow &&  
    \hat{D}(\hat{r})  
    = D(r) \frac{L^2}{T} \cdot \frac{\hat{T}}{\hat{L}^2} = D(r) \cdot \frac{t_{max}}{r_{max}^2}\\
    T_1(r) T = \hat{T}_1(\hat{r}) \hat{T} 
    && \Rightarrow &&
    \hat{T}_1(\hat{r}) =  T_1(r)  \frac{T}{\hat{T}}
    = T_1(r) \cdot \frac{1}{t_{max}}
\end{align*}

%\begin{align*}
%    \hat{R} =  R \times (\alpha  \cdot L) &= 1 \Rightarrow \alpha = \frac{1}{R \cdot L} \\
%    \hat{T}_{\text{max}} = T_{max} \times ( \beta \cdot T) &= 1 \Rightarrow \beta = \frac{1}{T_{max} \cdot T}
%\end{align*}
%
%
%
%
%Ce qui donne pour les autres variables :
%
%\begin{align*}
%    \hat{P}(\hat{r}, \hat{t}) &= P\left(\frac{\hat{r}}{\alpha}, \frac{\hat{t}}{\beta}\right) = P(r, t) \\
%    \hat{C}(\hat{r}) &= C\left(\frac{\hat{r}}{\alpha}\right) \frac{N}{(\alpha \cdot L)^3}  = \frac{C(r)}{\alpha^3} \times (N.L^{-3} )\\
%    \hat{D}(\hat{r}) &= D\left(\frac{\hat{r}}{\alpha}\right) \frac{(\alpha \cdot L)^2}{(\beta \cdot T)} = \frac{\alpha^2}{\beta}D(r) \times (L^2.T^{-1}) \\
%    \hat{T}_1(\hat{r}) &= T_1\left(\frac{\hat{r}}{\alpha}\right) \times (\beta \cdot T) = \beta . T_1(r) \times T \\
%\end{align*}


\subsection{Adimensionnalisation}

Nous adimensionnalisons les variables d'entrées et de sortie en posant :

\begin{align*}
    \bar{C}(\hat{r}) = \frac{\hat{C}(\hat{r})}{\hat{C}(r_{out})}\\
    %\bar{D}(\hat{r} < R) = 1 && \Rightarrow &&\bar{D}(\hat{r} \geq R) = \frac{\hat{D}(\hat{r} > R)}{\hat{D}(\hat{r} < R)}\\ 
    %\bar{T}_1(\hat{r} < R) = 1 && \Rightarrow &&\bar{T}_1(\hat{r} \geq R) = \frac{\hat{T}_1(\hat{r} > R)}{\hat{T}_1(\hat{r} < R)}\\
\end{align*}

En effet, en reprenant l'equation différentielle en coordonnées sphériques, nous obtenons :

\begin{align*}
    &C(r) \frac{\partial P(r, t)}{\partial t} = 
    \nabla \cdot \left(D(r) C(r) \frac{\partial}{\partial r} P(r, t)\right) 
    - C(r) \frac{P(r, t)-P_0(r)}{T_1(r)} 
    \\
    \Rightarrow
    & \bar{C}(\hat{r}) \frac{\partial \hat{P}(\hat{r}, \hat{t})}{\partial \hat{t}} = 
    \nabla \cdot \left(\hat{D}(\hat{r}) \bar{C}(\hat{r}) \frac{\partial}{\partial \hat{r}} \hat{P}(\hat{r}, \hat{t})\right) 
    - \bar{C}(\hat{r}) \frac{\hat{P}(\hat{r}, \hat{t})-\hat{P}_0(r_{out})}{\hat{T}_1(r_{out})} 
\end{align*}




Cela réduit encore l'espace de recherche en ne faisant varier que trois variables.


\subsection{Changement d'échelle}

Nous donnons au modèle le logarithme de chacune des valeurs
avec peut etre le racine $log\left(\hat{C}\right)$ et $log\left(\hat{D}\right)$

\subsection{normalisation}


A la suite de cela, nous appliquons une normalisation (loi normale multivariée) sur les variables d'entrées $\bar{C}(\hat{r})$, $\bar{D}(\hat{r})$, $\bar{T}_1(\hat{r})$ et de sortie $\hat{P}(\hat{r}, \hat{t})$ pour que l'apprentissage soit plus stable.



\end{document} 
--- END FILE: ./readme ---
--- START FILE: ./.gitignore ---
data/*
data_R/*

*.pyc
*/__pycache__/*
*.h5
*.xdmf
*.txt
--- END FILE: ./.gitignore ---
--- START FILE: ./experiments/fno2d/main.py ---
import os
import argparse

import torch
from torch import Tensor

from sklearn.model_selection import train_test_split

from neuralop.models import FNO

from experiments.fno2d.dataset import Dataset
from experiments.fno2d.integrator import plot_search_R
from experiments.fno2d.train import accuracy, G_accuracy, train



device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
match os.cpu_count():
    case None:  torch.set_num_threads(1)
    case n:     torch.set_num_threads(n)


parser = argparse.ArgumentParser(description="PDE solving.")
parser.add_argument("-l", "--n_layers", type=int, default=2, help="number of layers")
parser.add_argument("-m", "--n_modes", type=int, default=16, help="number of modes")
parser.add_argument("-c", "--hidden_channels", type=int, default=16, help="number of hidden channels")
parser.add_argument("-e", "--epochs", type=int, default=100, help="number of training epochs")
parser.add_argument("-p", "--model_path", type=str, default="", help="path to model")
parser.add_argument("-d", "--dataset_path", type=str, default="data", help="path to dataset")
parser.add_argument("-n", "--name", type=str, default="fno2d", help="name of the experiment")
parser.add_argument("-r", "--r_max_fixed", type=bool, default=False, help="if True, r_max is fixed to the true value")
args = parser.parse_args()
n_layers, n_modes, hidden_channels, epochs, model_path, dataset_path, name, r_max_fixed = args.n_layers, args.n_modes, args.hidden_channels, args.epochs, args.model_path, args.dataset_path, args.name, args.r_max_fixed


if __name__ == "__main__":
    dataset = Dataset()
    dataset.load(dataset_path)

    dataset.rescale()
    dataset.nondimensionalize()
    dataset.compress()
    dataset.normalize()

    train_dataset, test_dataset = Dataset(), Dataset()
    
    train_dataset.set_normalizers(dataset)
    test_dataset.set_normalizers(dataset)

    train_dataset.elements, test_dataset.elements = train_test_split(
        dataset.elements, 
        test_size=0.1, 
        random_state=42
    )

    train_dataloader = train_dataset.get_dataloader(64, shuffle=True)
    test_dataloader = test_dataset.get_dataloader(64, shuffle=False)


    if model_path:
        checkpoint = torch.load(args.model_path)
        model = FNO(**checkpoint["parameters"])
        model.load_state_dict(checkpoint["model_state_dict"])

    else:
        lift_dropout = 0.0
        projection_dropout = 0.0

        checkpoint = {
            "parameters": {
                "n_modes": (n_modes,n_modes),
                "hidden_channels": hidden_channels,
                "n_layers": n_layers,
                "lift_dropout": lift_dropout,
                "projection_dropout": projection_dropout,
            },

            "model_state_dict": None,
            "iterations": []
        }

        model = FNO(n_modes=(n_modes,n_modes),
                    hidden_channels=hidden_channels,
                    in_channels=3,
                    out_channels=1,
                    n_layers=n_layers,
                    lift_dropout=lift_dropout, 
                    projection_dropout=projection_dropout
        )

    model.to(device)

    print(f"Normalisation parameters: {train_dataset.C_normalizer=}, {train_dataset.D_normalizer=}, {train_dataset.T1_normalizer=}")

    print("Accuracy without training : ", accuracy(model, test_dataloader, device))
    print("G accuracy without training : ", G_accuracy(model, test_dataloader, device))


    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=1e-2,
        #betas=(0.9, 0.999),
        #eps=1e-8,
        weight_decay=1e-4
    )

    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5)

    train_losses, test_losses, times = train(model=model,
        dataloader=train_dataloader,
        optimizer=optimizer,
        scheduler=scheduler,
        epochs=epochs,
        device=device,
        test_loader=test_dataloader
    )

    checkpoint["iterations"].append({
        "train_losses": train_losses,
        "test_losses": test_losses,
        "times": times
    })

    checkpoint["model_state_dict"] = model.state_dict()

    os.makedirs("out/models", exist_ok=True)
    torch.save(checkpoint, f"out/models/{name}_{n_layers}_{n_modes}_{hidden_channels}_{len(checkpoint['iterations'])}.pth")


    print("Accuracy with training : ", accuracy(model, test_dataloader, device))
    print("G accuracy with training : ", G_accuracy:=G_accuracy(model, test_dataloader, device))

    for i in range(len(test_dataset.elements))[:10]:
        plot_search_R(model, test_dataset, i, device, f"out/plots/test_{name}_{i}", r_max_fixed=r_max_fixed)

    for i in range(len(train_dataset.elements))[:10]:
        plot_search_R(model, train_dataset, i, device, f"out/plots/train_{name}_{i}", r_max_fixed=r_max_fixed)
















--- END FILE: ./experiments/fno2d/main.py ---
--- START FILE: ./experiments/fno2d/edp_parameters.py ---
from __future__ import annotations

from collections.abc import Callable
import math

from torch import Tensor

class Normalizer:
    def __init__(self, mean: float, std: float):
        self.mean = mean
        self.std = std

    def normalize(self, value: float | Tensor) -> float | Tensor:
        return (value - self.mean) / self.std
    
    def unnormalize(self, value: float | Tensor) -> float | Tensor:
        return value * self.std + self.mean

    def __str__(self):
        return f"(mean={self.mean}, std={self.std})"

class EDPParameters:
    def __init__(self, 
                 Nr: int, Nt: int, 
                 R: float, r_max: float, 
                 t_max: float,
                 C_in: float, C_out: float,
                 D_in: float, D_out: float,
                 T1_in: float, T1_out: float,
                 P0_in: float, P0_out: float,
                 parent: EDPParameters | None = None,
                 parenthood_label: str | None = None,
    ):
        self.Nr = Nr
        self.Nt = Nt
        self.R = R
        self.r_max = r_max
        self.t_max = t_max
        self.C_in = C_in
        self.C_out = C_out
        self.D_in = D_in
        self.D_out = D_out
        self.T1_in = T1_in
        self.T1_out = T1_out
        self.P0_in = P0_in
        self.P0_out = P0_out

        self.parent = parent
        self.parenthood_label = parenthood_label
    
    
    def rescaling(self) -> "EDPParameters":
        t_max = 1.0  

        r_max = 1.0
        R = self.R * (1/self.r_max)

        C_in = self.C_in * (self.r_max**3)
        C_out = self.C_out * (self.r_max**3)
        
        D_in = self.D_in * (self.t_max/self.r_max**2)
        D_out = self.D_out * (self.t_max/self.r_max**2)

        T1_in = self.T1_in * (1/self.t_max)
        T1_out = self.T1_out * (1/self.t_max)

        P0_in = self.P0_in
        P0_out = self.P0_out        

        return EDPParameters(
            Nr=self.Nr, Nt=self.Nt,
            R=R, r_max=r_max, t_max=t_max,
            C_in=C_in, C_out=C_out,
            D_in=D_in, D_out=D_out,
            T1_in=T1_in, T1_out=T1_out,
            P0_in=P0_in, P0_out=P0_out,
            parent=self,
            parenthood_label="rescaling"
        )
    

    def nondimensionalize(self) -> "EDPParameters":
        
        C_out = 1.0
        C_in = self.C_in * (1/self.C_out)

        return EDPParameters(
            Nr=self.Nr, Nt=self.Nt,
            R=self.R, r_max=self.r_max, t_max=self.t_max,
            C_in=C_in, C_out=C_out,
            D_in=self.D_in, D_out=self.D_out,
            T1_in=self.T1_in, T1_out=self.T1_out,
            P0_in=self.P0_in, P0_out=self.P0_out,
            parent=self,
            parenthood_label="nondimensionalize"
        )


    def compression(self) -> "EDPParameters":
        def C_compress(C: float) -> float:
            return math.log(C)
        
        def D_compress(D: float) -> float:
            return math.log(D)
        
        def R_compress(R: float) -> float:
            return math.log(R)
        

        return EDPParameters(
            Nr=self.Nr, Nt=self.Nt,
            R=R_compress(self.R), r_max=self.r_max, 
            t_max=self.t_max,
            C_in=C_compress(self.C_in), C_out=C_compress(self.C_out),
            D_in=D_compress(self.D_in), D_out=D_compress(self.D_out),
            T1_in=self.T1_in, T1_out=self.T1_out,
            P0_in=self.P0_in, P0_out=self.P0_out,
            parent=self,
            parenthood_label="compression"
        )
    

    def normalize(self, C_normalizer: Callable[[float], float] | None = None, D_normalizer: Callable[[float], float] | None = None, R_normalizer: Callable[[float], float] | None = None, T1_normalizer: Callable[[float], float] | None = None) -> "EDPParameters":
        
        if C_normalizer is None:
            C_normalizer = lambda x: x
        if D_normalizer is None:
            D_normalizer = lambda x: x
        if R_normalizer is None:
            R_normalizer = lambda x: x
        if T1_normalizer is None:
            T1_normalizer = lambda x: x
        
        return EDPParameters(
            Nr=self.Nr, Nt=self.Nt,
            R=R_normalizer(self.R), r_max=self.r_max, 
            t_max=self.t_max,
            C_in=C_normalizer(self.C_in), C_out=C_normalizer(self.C_out),
            D_in=D_normalizer(self.D_in), D_out=D_normalizer(self.D_out),
            T1_in=self.T1_in, T1_out=self.T1_out,
            P0_in=self.P0_in, P0_out=self.P0_out,
            parent=self,
            parenthood_label="normalize"
        )

    

if __name__ == "__main__":
    params = EDPParameters(
        Nr=100, Nt=100,
        R=1.0, r_max=1.0, t_max=1.0,
        C_in=1.0, C_out=1.0,
        D_in=1.0, D_out=1.0,
        T1_in=1.0, T1_out=1.0,
        P0_in=1.0, P0_out=1.0
    )

    # basic usage
    params_to_learn = params.rescaling().nondimensionalize().compression().normalize()

    print(params_to_learn)
--- END FILE: ./experiments/fno2d/edp_parameters.py ---
--- START FILE: ./experiments/fno2d/integrator.py ---
from copy import deepcopy
from matplotlib import pyplot as plt
import torch
from torch import Tensor
from neuralop.models import FNO

from experiments.fno2d.dataset import Dataset, collate_fn
from experiments.fno2d.edp_parameters import EDPParameters


def compute_G(P_fno: Tensor, params: EDPParameters) -> Tensor:
    # P_fno: (B, pNt, pNr) or (pNt, pNr)
    was_unbatched = P_fno.ndim == 2
    if was_unbatched:
        P_fno = P_fno.unsqueeze(0)

    B, pNt, pNr = P_fno.shape
    Nt, Nr = pNt // 2, pNr // 2
    R, r_max = params.R, params.r_max

    q1 = P_fno[:, Nt:, Nr:]
    q2 = torch.flip(P_fno[:, Nt:, :Nr], dims=[-1])
    q3 = torch.flip(P_fno[:, :Nt, :Nr], dims=[-2, -1])
    q4 = torch.flip(P_fno[:, :Nt, Nr:], dims=[-2])

    P_quad_avg = (q1 + q2 + q3 + q4) / 4.0                   # (B, Nt, Nr)

    r_grid = torch.linspace(0, r_max, Nr, device=P_fno.device, dtype=P_fno.dtype)

    idx_R = torch.searchsorted(r_grid, R)
    r_sub = r_grid[:idx_R]                                  # (Nr_sub,)
    P_sub = P_quad_avg[..., :idx_R]                         # (B, Nt, Nr_sub)

    # Intégrale de G(R,t) = (3/R^3) * ∫[0,R] P(r,t) * r^2 dr
    integrand = P_sub * r_sub.pow(2)
    integral = torch.trapz(integrand, r_sub, dim=-1)        # (B, Nt)
    G = (3.0 / R**3) * integral if R > 1e-9 else torch.zeros_like(integral)

    return G.squeeze(0) if was_unbatched else G             # (Nt,) or (B, Nt)



def plot_search_R(model: FNO,
                  dataset: Dataset,
                  i: int,
                  device: torch.device,
                  name: str,
                  r_max_fixed: bool = False):
    
    with torch.no_grad():    
        model.eval()
        model.to(device)

        params_true, P_true = dataset.elements[i]

        params_tensored, P_tensored = collate_fn([dataset.elements[i]])
        P_tensored = P_tensored.to(device)

        G_true = compute_G(P_tensored, params_true)



        params_guess = deepcopy(params_true)

        linespace = torch.linspace(0.5 * params_true.R, 1.5 * params_true.R, 10, device=device)
        errors = []

        for R in linespace:
            params_guess.R = R.item()
            if not r_max_fixed:
                params_guess.r_max = params_true.r_max * R.item() / params_true.R

            params_tensored, _ = collate_fn([(params_guess, P_true)])

            P_pred = model(params_tensored)
            G_pred = compute_G(P_pred, params_guess)

            errors.append(torch.norm(G_pred - G_true).cpu().item())

        
        plt.plot(linespace.cpu().numpy(), errors)
        plt.savefig(f"out/plot/{name}.png")
        plt.xlabel("R")
        plt.ylabel("norm(G_pred - G_true)")
        plt.close()
--- END FILE: ./experiments/fno2d/integrator.py ---
--- START FILE: ./experiments/fno2d/train.py ---
import torch
import time

from neuralop import LpLoss
from experiments.fno2d.dataset import Dataset
from experiments.fno2d.integrator import compute_G_R_fno

lp_loss = LpLoss(d=2, p=2, reduction="mean") 

def accuracy(model: torch.nn.Module,
            dataloader: torch.utils.data.DataLoader,
            device: torch.device) -> float:

    model.eval()

    total_error: float = 0.0

    with torch.no_grad():
        for a, u in dataloader:
            a, u = a.to(device), u.to(device)

            u_pred = model(a).squeeze(1)  # Assuming model outputs shape (batch_size, 1, Nt, Nr)
            #error: torch.Tensor = torch.nn.functional.mse_loss(u, u_pred)

            error = lp_loss(u_pred, u)
            total_error += error.item()

    return total_error / len(dataloader)


def G_accuracy(model: torch.nn.Module,
               dataloader: torch.utils.data.DataLoader,
               device: torch.device) -> float:
    model.eval()
    total_error_G: float = 0.0
    g_loss = LpLoss(d=1, p=1, reduction="mean") # L1 Loss est souvent plus stable pour ce type de valeur

    # r_max devient 1.0 après le prétraitement .rescale() dans le dataset.
    r_max = 1.0

    with torch.no_grad():
        for a, u_true in dataloader:
            a, u_true = a.to(device), u_true.to(device)
            u_pred = model(a).squeeze(1)
            
            B, pNt, pNs = u_pred.shape
            Ns = pNs // 2
            dr = r_max / (Ns - 1) if Ns > 1 else 0.0

            # On utilise le canal 0 (carte de C) pour trouver la frontière R.
            c_map_quadrant_batch = a[:, 0, pNt // 2, Ns:]

            batch_g_pred = []
            batch_g_true = []

            for i in range(B):
                c_map = c_map_quadrant_batch[i]
                val_in = c_map[0]

                # Trouve l'index du premier point où la valeur du paramètre change.
                # torch.where est plus robuste que argmax.
                indices_changement = torch.where(c_map != val_in)[0]
                
                if len(indices_changement) == 0:
                    num_points_in_R = Ns
                else:
                    num_points_in_R = indices_changement[0].item()

                # Convertit le nombre de points en une distance physique (dans l'espace mis à l'échelle)
                R_scaled = num_points_in_R * dr

                # Appel à la bonne fonction d'intégration
                G_pred = compute_G_R_fno(u_pred[i:i+1], R_scaled, r_max)
                G_true = compute_G_R_fno(u_true[i:i+1], R_scaled, r_max)

                batch_g_pred.append(G_pred)
                batch_g_true.append(G_true)

            batch_g_pred_tensor = torch.cat(batch_g_pred)
            batch_g_true_tensor = torch.cat(batch_g_true)
            total_error_G += g_loss(batch_g_pred_tensor, batch_g_true_tensor).item()
            
    return total_error_G / len(dataloader)


def G_accuracy_old2(model: torch.nn.Module,
               dataloader: torch.utils.data.DataLoader,
               device: torch.device) -> float:
    
    model.eval()
    
    total_error_G: float = 0.0

    with torch.no_grad():
        for a, u_true in dataloader:
            a, u_true = a.to(device), u_true.to(device)
            
            u_pred = model(a).squeeze(1) # La prédiction du modèle, shape: (B, pNt, pNs)
            
            # Récupération des dimensions
            B, pNt, pNs = u_pred.shape 
            # Ns est la taille du domaine spatial original [0, r_max] (correspond à Nr+1 points)
            # L'axe spatial complet va de -r_max à +r_max, donc pNs = 2 * Ns
            # Le centre (r=0) se trouve à l'indice Ns
            Ns = pNs // 2

            # Le tenseur d'entrée 'a' (shape: B, 3, pNt, pNs) contient les cartes des paramètres C, D, T1.
            # On utilise le canal 0 (C) pour trouver la frontière R.
            # On observe une tranche du quadrant (t>0, r>0), par exemple à a[:, 0, pNt//2, Ns:].
            c_map_positive_r_batch = a[:, 0, pNt // 2, Ns:] # Shape: (B, Ns)
            
            batch_abs_error = 0.0
            for i in range(B):
                c_map = c_map_positive_r_batch[i]
                val_in = c_map[0] # Valeur du paramètre à l'intérieur du rayon R

                # On trouve l'indice du premier point où la valeur change.
                # (c_map != val_in) crée un tenseur de booléens (False/True).
                # .int().argmax() trouve l'index du premier True.
                first_change_idx = (c_map != val_in).int().argmax()

                # Si argmax renvoie 0, cela peut signifier soit que le changement est au premier indice,
                # soit qu'il n'y a aucun changement. On vérifie le dernier élément pour les distinguer.
                if first_change_idx == 0 and c_map[-1] == val_in:
                    # Aucun changement détecté, R couvre tout le domaine.
                    num_points_in_R = Ns
                else:
                    # Le nombre de points dans le rayon R sur l'axe positif.
                    num_points_in_R = first_change_idx.item()

                # Définition des bornes pour le slicing sur la bande [-R, R]
                # L'axe spatial est centré sur Ns.
                NR_min = Ns - num_points_in_R
                NR_max = Ns + num_points_in_R

                # On extrait la tranche correspondant à [-R, R]
                pred_slice = u_pred[i, :, NR_min:NR_max]
                true_slice = u_true[i, :, NR_min:NR_max]

                if pred_slice.numel() > 0:
                    G_pred = torch.mean(pred_slice)
                    G_true = torch.mean(true_slice)
                    batch_abs_error += torch.abs(G_pred - G_true)

            total_error_G += (batch_abs_error / B)
            
    return total_error_G.item() / len(dataloader)


def G_accuracy_(model: torch.nn.Module,
               dataloader: torch.utils.data.DataLoader,
               device: torch.device) -> float:
    
    model.eval()
    
    total_error_G: float = .0

    for a, u_true in dataloader:
        a, u_true = a.to(device), u_true.to(device)
        
        u_pred = model(a).squeeze(1)
        Nt, Nr = u_pred.shape[2], u_pred.shape[3]

        NR_min = 0
        NR_max = 0
        
        G_pred = torch.mean(u_pred[..., NR_min:NR_max, :])
        G_true = torch.mean(u_true[..., NR_min:NR_max, :])
        
        total_error_G += torch.mean(torch.abs(G_pred - G_true))
        
    return total_error_G / len(dataloader)



def G_accuracy_old(model: torch.nn.Module,
               dataset: Dataset,
               device: torch.device) -> float:
    
    model.eval()
    
    if not dataset.elements:
        return 0.0

    if dataset.param_mean is None or dataset.param_std is None:
        raise ValueError("Dataset is not normalized, cannot compute G accuracy.")
        
    total_error_G: float = 0.0
    g_loss = LpLoss(d=1, p=2, reduction="mean")

    r_max = dataset.elements[0][0].r_max
    R_mean = dataset.param_mean['R']
    R_std = dataset.param_std['R']

    dataloader = dataset.get_dataloader(bs=16, shuffle=False)
    
    with torch.no_grad():
        for a, u_true in dataloader:
            a, u_true = a.to(device), u_true.to(device)
            
            u_pred = model(a).squeeze(1)
            
            for i in range(a.shape[0]):
                log_R_norm = a[i, 3, 0, 0]
                log_R = log_R_norm * R_std + R_mean
                R = math.exp(log_R)

                G_pred = compute_G_R_fno(u_pred[i:i+1], R, r_max)
                G_true = compute_G_R_fno(u_true[i:i+1], R, r_max)

                total_error_G += g_loss(G_pred, G_true).item()
    
    return total_error_G / len(dataset)
 



def train_one_epoch(model: torch.nn.Module, 
                    dataloader: torch.utils.data.DataLoader, 
                    optimizer: torch.optim.Optimizer, 
                    device: torch.device) -> float:
    
    model.train()

    total_loss: float = 0.0

    for a, u in dataloader:
        a, u = a.to(device), u.to(device)

        optimizer.zero_grad()

        u_pred = model(a).squeeze(1)  # Assuming model outputs shape (batch_size, 1, Nt, Nr)

        #loss: torch.Tensor = torch.nn.functional.mse_loss(u, u_pred)
        loss = lp_loss(u_pred, u)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    
    total_loss = total_loss / len(dataloader)

    return total_loss


def train(model: torch.nn.Module, 
          dataloader: torch.utils.data.DataLoader, 
          optimizer: torch.optim.Optimizer, 
          scheduler: torch.optim.lr_scheduler._LRScheduler,
          epochs: int, 
          device: torch.device,
          test_loader: torch.utils.data.DataLoader | None) -> tuple[list[float], list[float], list[float]]:

    model.to(device)

    
    

    train_losses: list[float] = []
    times: list[float] = []
    test_losses: list[float] = []

    for epoch in range(epochs):
        t0: float = time.time()
        loss: float = train_one_epoch(model, dataloader, optimizer, device)
        t1: float = time.time()
        train_losses.append(loss)
        times.append(t1 - t0)

        if test_loader is not None:
            test_loss: float = accuracy(model, test_loader, device)
            test_losses.append(test_loss)
            
            # Mettre à jour le scheduler avec la perte de test
            scheduler.step(test_loss)
            
            current_lr = optimizer.param_groups[0]['lr']
            print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}, Test Loss: {test_loss:.4f}, LR: {current_lr:.6f}, Time: {t1 - t0:.2f}s")
        
        else:
            # Sans test_loader, ReduceLROnPlateau ne peut pas fonctionner.
            # On peut appeler step() sur la perte d'entraînement, mais c'est moins courant.
            scheduler.step(loss)
            current_lr = optimizer.param_groups[0]['lr']
            print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}, LR: {current_lr:.6f}, Time: {t1 - t0:.2f}s")

    return train_losses, test_losses, times

--- END FILE: ./experiments/fno2d/train.py ---
--- START FILE: ./experiments/fno2d/dataset.py ---
from __future__ import annotations

import os
import math 

import torch
from torch import Tensor

from experiments.fno2d.edp_parameters import EDPParameters, Normalizer


class Dataset(torch.utils.data.Dataset[tuple[EDPParameters, Tensor]]):

    def __init__(self):
        super().__init__()
        
        self.elements: list[tuple[EDPParameters, Tensor]] = []

        self.P_normalizer: Normalizer = Normalizer(mean=0.0, std=1.0)
        self.C_normalizer: Normalizer = Normalizer(mean=0.0, std=1.0)
        self.D_normalizer: Normalizer = Normalizer(mean=0.0, std=1.0)
        self.R_normalizer: Normalizer = Normalizer(mean=0.0, std=1.0)
        self.T1_normalizer: Normalizer = Normalizer(mean=0.0, std=1.0)
        self.P0_normalizer: Normalizer = Normalizer(mean=0.0, std=1.0)


    def add_element(self, d: dict[str, Tensor | float | int]):
        P= d["P"].clone() if isinstance(d["P"], Tensor) else torch.tensor(d["P"])

        Nr, Nt = int(d["Nr"]), int(d["Nt"])
        R, r_max = float(d["R"]), float(d["r_max"])
        t_max = float(d["Tfinal"])
        C_in, C_out = float(d["C_in"]), float(d["C_out"])
        D_in, D_out = float(d["D_in"]), float(d["D_out"])
        T1_in, T1_out = float(d["T1_in"]), float(d["T1_out"])
        P0_in, P0_out = float(d["P0_in"]), float(d["P0_out"])


        params = EDPParameters(
            Nr=Nr, Nt=Nt,
            R=R, r_max=r_max, t_max=t_max,
            C_in=C_in, C_out=C_out,
            D_in=D_in, D_out=D_out,
            T1_in=T1_in, T1_out=T1_out,
            P0_in=P0_in, P0_out=P0_out,
        )

        self.elements.append((params, P))


    def load(self, path:str):
        for f in os.listdir(path):
            if f.endswith(".pt"):
                self.add_element(torch.load(os.path.join(path, f), weights_only=False))


    def set_normalizers(self, dataset: Dataset):
        self.P_normalizer = Normalizer(mean=dataset.P_normalizer.mean, std=dataset.P_normalizer.std)
        self.C_normalizer = Normalizer(mean=dataset.C_normalizer.mean, std=dataset.C_normalizer.std)
        self.D_normalizer = Normalizer(mean=dataset.D_normalizer.mean, std=dataset.D_normalizer.std)
        self.R_normalizer = Normalizer(mean=dataset.R_normalizer.mean, std=dataset.R_normalizer.std)
        self.T1_normalizer = Normalizer(mean=dataset.T1_normalizer.mean, std=dataset.T1_normalizer.std)


    def normalize(self, dataset: Dataset | None = None):

        if dataset is not None:
            self.P_normalizer = Normalizer(mean=dataset.P_normalizer.mean, std=dataset.P_normalizer.std)
            self.C_normalizer = Normalizer(mean=dataset.C_normalizer.mean, std=dataset.C_normalizer.std)
            self.D_normalizer = Normalizer(mean=dataset.D_normalizer.mean, std=dataset.D_normalizer.std)
            self.R_normalizer = Normalizer(mean=dataset.R_normalizer.mean, std=dataset.R_normalizer.std)
            self.T1_normalizer = Normalizer(mean=dataset.T1_normalizer.mean, std=dataset.T1_normalizer.std)

        else:
            acc = {"C":0.,"D":0.,"T1":0., "P0":0., "R":0.}
            acc_sq = {"C":0.,"D":0.,"T1":0., "P0":0., "R":0.}
            P_sum = P_sq = 0.0
            P_count = 0

            for p, P in self.elements:
                li, lo = p.R, p.r_max - p.R

                acc["R"] += p.R
                acc["C"]  += p.C_in * li + p.C_out * lo
                acc["D"]  += p.D_in * li + p.D_out * lo
                acc["T1"] += p.T1_in * li + p.T1_out * lo
                acc["P0"] += p.P0_in * li + p.P0_out * lo

                acc_sq["R"] += p.R**2
                acc_sq["C"]  += p.C_in**2  * li + p.C_out**2  * lo
                acc_sq["D"]  += p.D_in**2  * li + p.D_out**2  * lo
                acc_sq["T1"] += p.T1_in**2 * li + p.T1_out**2 * lo
                acc_sq["P0"] += p.P0_in**2 * li + p.P0_out**2 * lo

                P_sum += P.sum().item()
                P_sq  += (P*P).sum().item()
                P_count += P.numel()

            self.C_normalizer.mean = acc["C"]  / len(self.elements)
            self.D_normalizer.mean = acc["D"]  / len(self.elements)
            self.T1_normalizer.mean = acc["T1"] / len(self.elements)
            self.P0_normalizer.mean = acc["P0"] / len(self.elements)
            self.R_normalizer.mean = acc["R"]  / len(self.elements)

            self.C_normalizer.std = math.sqrt(max(acc_sq["C"]/(len(self.elements)) - self.C_normalizer.mean**2, 1e-8))
            self.D_normalizer.std = math.sqrt(max(acc_sq["D"]/(len(self.elements)) - self.D_normalizer.mean**2, 1e-8))
            self.T1_normalizer.std = math.sqrt(max(acc_sq["T1"]/(len(self.elements)) - self.T1_normalizer.mean**2, 1e-8))
            self.P0_normalizer.std = math.sqrt(max(acc_sq["P0"]/(len(self.elements)) - self.P0_normalizer.mean**2, 1e-8))
            self.R_normalizer.std = math.sqrt(max(acc_sq["R"]/(len(self.elements)) - self.R_normalizer.mean**2, 1e-8))

            self.P_normalizer.mean = P_sum / P_count
            self.P_normalizer.std = math.sqrt(max(P_sq / P_count - self.P_normalizer.mean**2, 1e-8))


        for p, P in self.elements:
            P = self.P_normalizer.normalize(P) # type: ignore

            p.C_in, p.C_out = self.C_normalizer.normalize(p.C_in), self.C_normalizer.normalize(p.C_out) # type: ignore
            p.D_in, p.D_out = self.D_normalizer.normalize(p.D_in), self.D_normalizer.normalize(p.D_out) # type: ignore
            p.T1_in, p.T1_out = self.T1_normalizer.normalize(p.T1_in), self.T1_normalizer.normalize(p.T1_out) # type: ignore
            p.P0_in, p.P0_out = self.P0_normalizer.normalize(p.P0_in), self.P0_normalizer.normalize(p.P0_out) # type: ignore

            p.R = self.R_normalizer.normalize(p.R) # type: ignore


    def rescale(self):
        for p, _ in self.elements:
            p = p.rescaling()


    def nondimensionalize(self):
        for p, _ in self.elements:
            p = p.nondimensionalize()


    def compress(self):
        for p, _ in self.elements:
            p = p.compression()
      

    def plot_element(self, idx:int, normalised:bool=True):
        import copy
        import matplotlib.pyplot as plt
        
        if idx >= len(self.elements):
            return

        params, p_quadrant = self.elements[idx]
        p_quadrant = p_quadrant.clone()

        plot_params = params
        if not normalised:
            plot_params = copy.deepcopy(params)
            plot_params.C_in = self.C_normalizer.unnormalize(params.C_in)
            plot_params.C_out = self.C_normalizer.unnormalize(params.C_out)
            plot_params.D_in = self.D_normalizer.unnormalize(params.D_in)
            plot_params.D_out = self.D_normalizer.unnormalize(params.D_out)
            plot_params.T1_in = self.T1_normalizer.unnormalize(params.T1_in)
            plot_params.T1_out = self.T1_normalizer.unnormalize(params.T1_out)
            plot_params.P0_in = self.P0_normalizer.unnormalize(params.P0_in)
            plot_params.P0_out = self.P0_normalizer.unnormalize(params.P0_out)
            plot_params.R = self.R_normalizer.unnormalize(params.R)
            p_quadrant = self.P_normalizer.unnormalize(p_quadrant)

        Nr, Nt = plot_params.Nr, plot_params.Nt
        r_max, R = plot_params.r_max, plot_params.R
        Ns = Nr + 1
        n_in_points = int(Ns * R / r_max) if r_max > 0 else 0

        c_map = torch.full((Nt, Ns), plot_params.C_out)
        c_map[:, :n_in_points] = plot_params.C_in
        
        d_map = torch.full((Nt, Ns), plot_params.D_out)
        d_map[:, :n_in_points] = plot_params.D_in
        
        t1_map = torch.full((Nt, Ns), plot_params.T1_out)
        t1_map[:, :n_in_points] = plot_params.T1_in
        
        p0_map = torch.full((Nt, Ns), plot_params.P0_out)
        p0_map[:, :n_in_points] = plot_params.P0_in

        r_map = torch.full((Nt, Ns), plot_params.R)

        def apply_symmetry(quadrant):
            q_h_flip = torch.flip(quadrant, (0,))
            row1 = torch.cat((torch.flip(q_h_flip, (1,)), q_h_flip), dim=1)
            q_w_flip = torch.flip(quadrant, (1,))
            row2 = torch.cat((q_w_flip, quadrant), dim=1)
            return torch.cat((row1, row2), dim=0)

        data_maps = [apply_symmetry(m) for m in [c_map, d_map, t1_map, p0_map, r_map, p_quadrant]]
        titles = ["C", "D", "T1", "P0", "R", "P"]

        fig, axs = plt.subplots(2, 3, figsize=(16, 10))
        for ax, t, d in zip(axs.flat, titles, data_maps):
            im = ax.imshow(d.cpu().numpy(), origin="lower", aspect="auto", cmap="viridis")
            ax.set_title(t)
            fig.colorbar(im, ax=ax)
        plt.tight_layout()
        plt.savefig(f"element_{idx}_{'norm' if normalised else 'denorm'}.png", dpi=180)
        plt.close(fig)




    def __len__(self):
        return len(self.elements)
    
    def __getitem__(self, idx: int):
        return self.elements[idx]
    

    def get_dataloader(self, bs:int, shuffle:bool=True):
        return torch.utils.data.DataLoader(
            self, batch_size=bs, shuffle=shuffle, collate_fn=collate_fn
        )
    

def collate_fn(batch:list[tuple[EDPParameters, Tensor]]):
    if not batch:
        return torch.empty(0), torch.empty(0)
    
    p0, P0 = batch[0]
    B = len(batch)
    Nt = p0.Nt
    Ns = p0.Nr + 1
    pNt, pNs = 2 * Nt, 2 * Ns

    device, dtype = P0.device, P0.dtype
    P_out = torch.empty((B, pNt, pNs), device=device, dtype=dtype)
    params_out = torch.empty((B, 3, pNt, pNs), device=device, dtype=dtype)  # 3 canaux: C, D, T1

    for b, (p, P) in enumerate(batch):
        # Target P (symétrie des quadrants)
        P_out[b, Nt:, Ns:] = P
        P_out[b, Nt:, :Ns] = torch.flip(P, dims=(-1,))
        P_out[b, :Nt, :] = torch.flip(P_out[b, Nt:, :], dims=(0,))

        # Paramètres C, D, T1
        n_in_points = int(Ns * p.R / p.r_max) if p.r_max > 0 else 0
        
        br = params_out[b, :, Nt:, Ns:]  # Vue sur le quadrant positif

        br[0, :, :n_in_points] = p.C_in
        br[0, :, n_in_points:] = p.C_out
        
        br[1, :, :n_in_points] = p.D_in
        br[1, :, n_in_points:] = p.D_out
        
        br[2, :, :n_in_points] = p.T1_in
        br[2, :, n_in_points:] = p.T1_out

        params_out[b, :, Nt:, :Ns] = torch.flip(br, dims=(-1,))
        params_out[b, :, :Nt, :] = torch.flip(params_out[b, :, Nt:, :], dims=(1,))

    return params_out, P_out
    
    
--- END FILE: ./experiments/fno2d/dataset.py ---
--- START FILE: ./utils_/save.py ---
from typing import Any, cast
import json
import os
import numpy as np

class NumpyEncoder(json.JSONEncoder):
    def default(self, o: Any) -> Any:
        if isinstance(o, np.integer):
            return int(o)
        elif isinstance(o, np.floating):
            return float(o)
        elif isinstance(o, np.ndarray):
            return o.tolist()
        return super().default(o)

# save to format js line
def save_result(path: str, results: dict[str, Any]):
    results_str = json.dumps(results, cls=NumpyEncoder)
    
    # create directory if it does not exist
    os.makedirs(os.path.dirname(path), exist_ok=True)

    with open(path, 'a') as f:
        f.write(results_str + '\n')
    


def load_results(path: str) -> list[dict[str, Any]]:
    list_results: list[dict[str, Any]] = []
    with open(path, 'r') as f:
        for line in f:
            ljs = json.loads(line)

            if not isinstance(ljs, dict):
                raise ValueError(f"Invalid line in file: {line}")
            
            typed_ljs = cast(dict[str, Any], ljs)
            list_results.append(typed_ljs)
    return list_results


def save_results(path: str, results: list[dict[str, Any]]):
    os.makedirs(os.path.dirname(path), exist_ok=True)

    with open(path, 'a') as f:
        for result in results:
            result_str = json.dumps(result)
            f.write(result_str + '\n')


def conc_files(input_file: str, output_file: str):
    with open(input_file, 'r') as f_in, open(output_file, 'a') as f_out:
        for line in f_in:
            f_out.write(line)
--- END FILE: ./utils_/save.py ---
--- START FILE: ./utils_/database.py ---
import os
from util.data_generator import DataGenerator
from util.convert import C_converter, R_converter, D_converter

from typing import Any
import random as rd
import numpy as np
import math
import torch

def empty_database(folder: str):
    if os.path.exists(folder):
        for sub_folder in ["train", "test", "dev", "plot"]:
            sub_folder_path = os.path.join(folder, sub_folder)
            if os.path.exists(sub_folder_path):
                for file in os.listdir(sub_folder_path):
                    os.remove(os.path.join(sub_folder_path, file))
            else:
                os.makedirs(sub_folder_path)
    else:
        os.makedirs(folder)
        os.makedirs(os.path.join(folder, "train"))
        os.makedirs(os.path.join(folder, "test"))
        os.makedirs(os.path.join(folder, "dev"))
        os.makedirs(os.path.join(folder, "plot"))


def extract_params_from_brut(folder: str):
    list_params: list[dict[Any, Any]] = []
    for sub_folder in os.listdir(folder):

        if not os.path.isdir(os.path.join(folder, sub_folder)):
            continue

        try:
            C_out, C_in, T1_in, R_nm = map(float, sub_folder.split('_'))
        except ValueError:
            print(f"Skipping folder {sub_folder} due to ValueError")
            continue


        for file in os.listdir(os.path.join(folder, sub_folder)):

            P0, TB, Time, P = np.loadtxt(
                os.path.join(folder, sub_folder, file),
                comments='%',          # saute toutes les lignes qui commencent par %
                unpack=True            # renvoie 4 tableaux séparés
            )

            P0 = float(P0[0])
            TB = float(TB[0])


            # Convertir les valeurs
            C_out = C_converter(C_out)  # C_out from mol.L^-1 to Fmol.µm^3
            C_in = C_converter(C_in)  # C_in from mol.L^-1 to Fmol.µm^3

            R = R_converter(R_nm)  # R in µm

            D_ref = D_converter(500.)  # D_ref = 500 nm^2.s^-1
            C_ref = C_converter(60.)   # C_ref = 60 mol.L^-1      
            D_in = D_ref * (C_in / C_ref) ** (1/3)
            D_out = D_ref * (C_out/ C_ref) ** (1/3)


            T1_out = TB

            list_params.append({
                "C_out": C_out,
                "C_in": C_in,
                "D_out": D_out,
                "D_in": D_in,
                "T1_out": T1_out,
                "T1_in": T1_in,
                "P0_out": P0,
                "P0_in": 1., # car Boltzmann est 1
                "R": R, 
            })

    return list_params


def create_database(list_dict: list[dict[Any, Any]], folder: str, n=20, micro_ondes: bool= True, test: float = 0.1, dev: float = 0.1):

    R_max = max([params["R"] for params in list_dict]) # typ: ignore
    R_min = min([params["R"] for params in list_dict]) # typ: ignore

    C_out_max = max([params["C_out"] for params in list_dict]) # typ: ignore
    C_out_min = min([params["C_out"] for params in list_dict]) # typ: ignore
    C_in_max = max([params["C_in"] for params in list_dict]) # typ: ignore
    C_in_min = min([params["C_in"] for params in list_dict]) # typ: ignore

    D_out_max = max([params["D_out"] for params in list_dict]) # typ: ignore
    D_out_min = min([params["D_out"] for params in list_dict]) # typ: ignore
    D_in_max = max([params["D_in"] for params in list_dict]) # typ: ignore
    D_in_min = min([params["D_in"] for params in list_dict]) # typ: ignore

    T1_in_max = max([params["T1_in"] for params in list_dict]) # typ: ignore
    T1_in_min = min([params["T1_in"] for params in list_dict]) # typ: ignore

    #P0_out_max = max([params["P0_out"] for params in list_dict])
    #P0_out_min = min([params["P0_out"] for params in list_dict])
    #T1_out_max = max([params["T1_out"] for params in list_dict])
    #T1_out_min = min([params["T1_out"] for params in list_dict])
    #P0_in_max = max([params["P0_in"] for params in list_dict])
    #P0_in_min = min([params["P0_in"] for params in list_dict])


    if micro_ondes:
        P0_out = 100
        T1_out = 3.
    
    else:
        P0_out = 0.5
        T1_out = 3.5


    print(f"""{R_min} < R < {R_max} 
            {C_in_min} < C_in < {C_in_max:2f}, {C_out_min} < C_out < {C_out_max:2f} 
            {D_in_min:2f} < D_in < {D_in_max:2f}, {D_out_min:2f} < D_out < {D_out_max:2f} 
            {T1_in_min:2f} < T1_in < {T1_in_max:2f}, T1_out = {T1_out:2f} 
            P0_in = 1, P0_out = {P0_out:2f} 
    """)


    for _ in range(n):
        R =  rd.uniform(0.001, 0.5) 
        #rd.uniform(R_min, R_max)
        #math.exp(rd.uniform(math.log(R_min), math.log(R_max)))
        dg = DataGenerator(
            R=R,
            r_max=6*R,
            C_in=rd.uniform(C_in_min, C_in_max),
            C_out=rd.uniform(C_out_min, C_out_max),
            D_in=rd.uniform(D_in_min, D_in_max),
            D_out=rd.uniform(D_out_min, D_out_max),
            T1_in=rd.uniform(T1_in_min, T1_in_max),
            T1_out=T1_out,  #rd.uniform(T1_out_min, T1_out_max),
            P0_in=1., #rd.uniform(P0_in_min, P0_in_max),
            P0_out=P0_out, #rd.uniform(P0_out_min, P0_out_max),
            Tfinal=40.,
            Nr=200,
            Nt=100,
            tanh_slope=0
        )
        dg.solve()
        dg.plot(os.path.join(folder, "plot"))
        data = dg.get()

        if (a:=rd.random()) < test:
            sub_folder = "test"
        elif a < test + dev:
            sub_folder = "dev"
        else:
            sub_folder = "train"

        file_path = os.path.join(folder, sub_folder, f"data_{data['C_out']}_{data['C_in']}_{data['D_in']}_{data['D_out']}_{data['R']}.pt")
        torch.save(data, file_path)

        data["P"] = []
        print(f"{data=}")
        

        del dg
        del data

        
if __name__ == "__main__":
    #empty_database("data")
    list_dict = extract_params_from_brut("data/brut")
    create_database(list_dict, "data_R", test=0.1, dev=0.1, n=10_000, micro_ondes=True)
--- END FILE: ./utils_/database.py ---
--- START FILE: ./utils_/convert.py ---
def D_converter(D: float) -> float:
    """
    Convert D from nm^2.s^-1 to µm^2.s^-1
    """
    return D * 1e-6

def C_converter(C: float) -> float:
    """
    Convert C from mol.L^-1 to Fmol.µm^3
    """
    return C

def R_converter(R: float) -> float:
    """
    Convert R from nm to µm
    """
    return R * 1e-3
--- END FILE: ./utils_/convert.py ---
--- START FILE: ./utils_/data_generator.py ---
from mpi4py import MPI
import numpy as np
import ufl
from petsc4py.PETSc import ScalarType
from dolfinx import mesh, fem
from dolfinx.fem.petsc import LinearProblem
import torch
from torch import Tensor
from collections.abc import Callable
import os


class DataGenerator:
    def __init__(self,
                 R: float, r_max: float,
                 C_in: float, C_out: float,
                 D_in: float, D_out: float,
                 T1_in: float, T1_out: float,
                 P0_in: float, P0_out: float,
                 Tfinal: float = 10.,
                 Nr: int = 100,
                 Nt: int = 100,
                 tanh_slope: float = 0.,
                 ):

        self.R = R
        self.r_max = r_max
        self.Tfinal = Tfinal
        self.Nr = Nr
        self.Nt = Nt
        self.dt = self.Tfinal / self.Nt
        self.tanh_slope = tanh_slope

        self.C_in = C_in
        self.C_out = C_out
        self.D_in = D_in
        self.D_out = D_out
        self.T1_in = T1_in
        self.T1_out = T1_out
        self.P0_in = P0_in
        self.P0_out = P0_out

        if tanh_slope == 0.:
            self.C: Callable = lambda r: ufl.conditional(ufl.lt(r, self.R), C_in, C_out)
            self.D: Callable = lambda r: ufl.conditional(ufl.lt(r, self.R), D_in, D_out)
            self.T1: Callable = lambda r: ufl.conditional(ufl.lt(r, self.R), T1_in, T1_out)
            self.P0: Callable = lambda r: ufl.conditional(ufl.lt(r, self.R), P0_in, P0_out)
        else:
            mid = 0.5 * (C_out + C_in)
            amp = 0.5 * (C_out - C_in)
            self.C = lambda r: mid + amp * ufl.tanh((r - self.R) / tanh_slope)
            mid = 0.5 * (D_out + D_in)
            amp = 0.5 * (D_out - D_in)
            self.D = lambda r: mid + amp * ufl.tanh((r - self.R) / tanh_slope)
            mid = 0.5 * (T1_out + T1_in)
            amp = 0.5 * (T1_out - T1_in)
            self.T1 = lambda r: mid + amp * ufl.tanh((r - self.R) / tanh_slope)
            mid = 0.5 * (P0_out + P0_in)
            amp = 0.5 * (P0_out - P0_in)
            self.P0 = lambda r: mid + amp * ufl.tanh((r - self.R) / tanh_slope)

        self.msh = None
        self.V = None
        self.P_time = []
        self.r_sorted = None
        self.t_vec = None

    def solve(self):
        self.msh = mesh.create_interval(MPI.COMM_WORLD, self.Nr, [0.0, self.r_max])
        self.V = fem.functionspace(self.msh, ("Lagrange", 1))
        x = ufl.SpatialCoordinate(self.msh)[0]
        r = x
        w = r**2
        C_expr = self.C(r)
        D_expr = self.D(r)
        T1_expr = self.T1(r)
        P0_expr = self.P0(r)
        P_n1 = ufl.TrialFunction(self.V)
        v = ufl.TestFunction(self.V)
        P_n = fem.Function(self.V)
        P_n.x.array[:] = 0.0
        self.P_time.append(P_n)
        dx = ufl.dx
        mass_lhs = (C_expr / ScalarType(self.dt)) * w * P_n1 * v * dx
        mass_rhs = (C_expr / ScalarType(self.dt)) * w * P_n * v * dx
        diff_lhs = D_expr * C_expr * w * ufl.dot(ufl.grad(P_n1), ufl.grad(v)) * dx
        react_lhs = (C_expr / T1_expr) * w * P_n1 * v * dx
        react_rhs = (C_expr / T1_expr) * w * P0_expr * v * dx
        a_form = mass_lhs + diff_lhs + react_lhs
        L_form = mass_rhs + react_rhs
        problem = LinearProblem(
            a_form, L_form, [],
            petsc_options={"ksp_type": "preonly", "pc_type": "lu"}
        )
        n_loc = self.V.dofmap.index_map.size_local
        r_loc = self.V.tabulate_dof_coordinates()[:n_loc, 0]
        r_all = self.msh.comm.gather(r_loc, root=0)
        if self.msh.comm.rank == 0:
            r_glob = np.concatenate(r_all)
            order = np.argsort(r_glob)
            self.r_sorted = r_glob[order]
            P_hist = []
        t = 0.0
        for _ in range(self.Nt):
            t += self.dt
            P_new = problem.solve()
            P_loc = P_new.x.array[:n_loc]
            P_all = self.msh.comm.gather(P_loc, root=0)
            if self.msh.comm.rank == 0:
                P_glob = np.concatenate(P_all)
                P_hist.append(P_glob[order].copy())
            P_n.x.array[:] = P_new.x.array
        if self.msh.comm.rank == 0:
            self.P_time = np.array(P_hist)
            self.t_vec = np.linspace(self.dt, self.Tfinal, self.Nt)

    def plot(self, dir: str = "data/plot"):
        if self.msh.comm.rank == 0:
            if self.P_time is None or self.r_sorted is None or self.t_vec is None:
                return
            import matplotlib.pyplot as plt
            os.makedirs(dir, exist_ok=True)
            plt.figure(figsize=(8, 5))
            plt.imshow(self.P_time,
                       extent=[0.0, self.r_max, 0.0, self.Tfinal],
                       origin="lower",
                       aspect="auto",
                       cmap="viridis")
            plt.colorbar(label=r"$P(r,t)$")
            plt.xlabel(r"$r$ (m)")
            plt.ylabel(r"$t$ (s)")
            plt.title("Évolution de la polarisation P(r,t)")
            plt.tight_layout()
            filename = f"{dir}/{self.R}_{self.C_in}_{self.C_out}_{self.D_in}_{self.D_out}_{self.P0_in}_{self.P0_out}_{self.T1_in}_{self.T1_out}.png"
            plt.savefig(filename, dpi=180)

    def get(self) -> dict[str, Tensor | float | int]:
        if self.msh.comm.rank == 0:
            P_tensor = torch.tensor(self.P_time, dtype=torch.float32)
            r = self.r_sorted
            idx_cut = np.searchsorted(r, self.R, side='right')
            G_vals = []
            for line in self.P_time:
                r_sub = r[:idx_cut]
                P_sub = line[:idx_cut]
                if r_sub[-1] < self.R:
                    if idx_cut < len(r):
                        r1, r2 = r[idx_cut - 1], r[idx_cut]
                        P1, P2 = line[idx_cut - 1], line[idx_cut]
                        P_R = P1 + (P2 - P1) * (self.R - r1) / (r2 - r1)
                    else:
                        P_R = P_sub[-1]
                    r_sub = np.append(r_sub, self.R)
                    P_sub = np.append(P_sub, P_R)
                G = 3.0 / (self.R ** 3) * np.trapz(P_sub * (r_sub ** 2), r_sub)
                G_vals.append(G)
            G_tensor = torch.tensor(G_vals, dtype=torch.float32)
            to_send: dict[str, Tensor | float | int] = {
                "P": P_tensor,
                "G_R": G_tensor,
                "r_max": self.r_max,
                "Tfinal": self.Tfinal,
                "Nr": self.Nr,
                "Nt": self.Nt,
                "dt": self.dt,
                "C_in": self.C_in,
                "C_out": self.C_out,
                "D_in": self.D_in,
                "D_out": self.D_out,
                "T1_in": self.T1_in,
                "T1_out": self.T1_out,
                "P0_in": self.P0_in,
                "P0_out": self.P0_out,
                "R": self.R
            }
            return to_send
        return {}

--- END FILE: ./utils_/data_generator.py ---
